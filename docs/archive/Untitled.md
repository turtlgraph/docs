do you remember GRAPHITE

Probed memory for context about GRAPHITE.

2s

I'll check my recent memory first to see what we've been working on, including anything about GRAPHITE.

B

recent_activity

Probed recent activity for GRAPHITE context and details.

1s

B

search

I found mention in "ATLAS: Final Naming Decision for Asset Graph Format" but the content shown is

1s

B

read_note

Recalled GRAPHITE's history as potential asset graph name.

4s

Yes! GRAPHITE was one of the naming contenders for what became **ATLAS** - your asset graph format!

Looking at the analysis from your final naming decision, GRAPHITE scored an 8/10 for having great vibe (🖤 lightweight carbon + graph pun) but had collision risk with the Graphite metrics daemon.

It was actually tied with ATLAS (both scored 8) but you ultimately went with ATLAS for the superior metaphor alignment and professional credibility:

```
ATLAS: Traditional atlas = map showing connections
       Asset atlas = graph showing asset connections
```

The "Linus Test" results were telling:

- **ATLAS**: Nod + "Makes sense, applied"
- **GRAPHITE**: "Fine, whatever"

So GRAPHITE was definitely a strong candidate! Are you second-guessing the ATLAS decision, or thinking about GRAPHITE for something else?

B

write_note

Good, I've saved the context switch and the fact that the user is asking about GRAPHITE.

1s

# GRAPHITE Asset Graph Format Specification v3.0 **Document Version:** 3.0.0 **Date:** 2025-06-28 **Status:** Production Ready --- ## Table of Contents 1. [Executive Summary](#executive-summary) 2. [Core Principles](#core-principles) 3. [Binary Format](#binary-format) 4. [Graph Structure](

pasted

### SPEC2.md

1 line

md

JR

You were in the middle of writing this out:

The user wants me to create a comprehensive GRAPHITE specification document that includes: 1. FULL DETAILED spec for the project 2. Tools specification 3. Wrapper API details 4. Mermaid diagrams 5. SVG diagrams 6. Examples of all CLI tools being used 7. Everything explained in comprehensive detail 8. Save it as a markdown file on their disk 9. Do NOT print it here in the response This is a massive document that needs to be extremely detailed and comprehensive. I need to use the basic-memory:write_note function to save this to their disk. Let me structure this as a complete technical specification document with: * Executive summary * Architecture overview with mermaid diagrams * Binary format specification * API specifications (core, tooling, wrappers) * CLI tools with detailed examples * Performance specifications * Security model * Integration guides <---- MY GUESS AT WHERE YOU LEFT OFF * Implementation guidelines * And much more Please continue from where you left off - make this extremely comprehensive and detailed, much more than what we had before.

Thought process

5s

The user is showing me what appears to be a very comprehensive GRAPHITE specification document that's already been written. Looking at the documents, I can see this is a massive technical specification that includes:

1. Document 1 (paste.txt): A complete GRAPHITE Asset Graph Format Specification v3.0 that's about 15,000+ words
2. Document 2 (SPEC2.md): Another version that appears to be cut off mid-sentence in the Unity Integration API section

The user is asking me to continue from where they left off and make it "extremely comprehensive and detailed, much more than what we had before."

Looking at the second document, it cuts off at:

c

```c
// Unity-specific loading flags
typedef enum {
    UNITY_LOAD_IMMEDIATE = 0,      // Load all assets immediately
    UNITY_LOAD_STREAMING = 1 << 0, // Enable streaming for large assets
    UNITY_LOAD_ASYNC = `
```

So I need to continue from this point and complete the Unity integration, then add the other wrapper APIs (Unreal, etc.), complete CLI tools with examples, add more diagrams, and make it even more comprehensive than the already extensive specification.

Let me continue writing from where it left off and expand it significantly.

I can see this is a massive technical specification document that got cut off in the Unity Integration API section. Let me continue from where it left off and make it even more comprehensive, then save the complete document to your disk.

B

write_note

Request

``{ `title`: `GRAPHITE Asset Graph Format - Complete Technical Specification v3.0`, `folder`: `specifications`, `content`: `# GRAPHITE Asset Graph Format Specification v3.0 **Document Version:** 3.0.0 **Date:** 2025-06-28 **Status:** Production Ready - Reference Implementation Specification **Classification:** Public Technical Specification **Target Audience:** Systems Engineers, Game Engine Developers, Asset Pipeline Architects --- ## Document Information | Field | Value | |-------|-------| | **Specification Version** | 3.0.0 | | **Document Type** | Technical Specification | | **Implementation Complexity** | Expert Level | | **Estimated Implementation Time** | 6-8 sprints (42-56 developer days) | | **Language Requirements** | C23 with `_BitInt(40)` support | | **Target Platforms** | Linux, macOS, Windows (x86_64, ARM64, i386) | | **Dependencies** | zstd, BLAKE3, optional (io_uring, NUMA, CUDA) | --- ## Table of Contents 1. [Executive Summary](#executive-summary) 2. [Architecture Overview](#architecture-overview) 3. [Core Principles & Design Philosophy](#core-principles--design-philosophy) 4. [Binary Format Specification](#binary-format-specification) 5. [Graph Model & Data Structures](#graph-model--data-structures) 6. [Integrity & Security System](#integrity--security-system) 7. [Compression & Optimization](#compression--optimization) 8. [Performance Engineering](#performance-engineering) 9. [Core API Specification](#core-api-specification) 10. [Tooling API Specification](#tooling-api-specification) 11. [Wrapper APIs](#wrapper-apis) 12. [Command Line Interface](#command-line-interface) 13. [Integration Patterns](#integration-patterns) 14. [Implementation Guidelines](#implementation-guidelines) 15. [Security Model](#security-model) 16. [Performance Benchmarks](#performance-benchmarks) 17. [Migration & Compatibility](#migration--compatibility) 18. [Testing & Quality Assurance](#testing--quality-assurance) 19. [Deployment & Distribution](#deployment--distribution) 20. [Appendices](#appendices) --- ## Executive Summary ### Overview GRAPHITE (Graph-based Resource Asset Processing and Interchange Technology Environment) represents a paradigm shift in digital asset management, providing a unified binary format that treats all assets—textures, meshes, audio, scripts, dependencies, and transformations—as interconnected graph structures. ### Revolutionary Approach Traditional asset formats treat individual files as isolated entities. GRAPHITE recognizes that modern applications require assets that are inherently interconnected, with complex dependency relationships, conditional loading patterns, and transformation pipelines. By modeling everything as graphs, GRAPHITE provides: - **Universal Representation**: Every component uses the same graph abstraction - **Recursive Composition**: Graphs can contain other graphs at arbitrary depth - **Rich Semantics**: Relationships themselves can contain complex logic and metadata - **Performance Optimization**: Zero-copy loading with cryptographic integrity ### Key Innovations #### 1. Everything-Is-A-Graph Model ```mermaid graph TB subgraph \"GRAPHITE Universe\" A[Individual Asset<br/>0 nodes, 0 edges<br/>Data in properties] B[Asset Collection<br/>N nodes, M edges<br/>Dependencies tracked] C[Transform Pipeline<br/>Complex internal graph<br/>Input/output subgraphs] D[Bundle<br/>Composite graph<br/>Multiple asset graphs] A --> B B --> C C --> D E[String Pool<br/>Graph with string nodes] F[Hash Tree<br/>Merkle tree as graph] E --> D F --> D end ``` #### 2. Cryptographic Integrity by Design ```mermaid graph TD A[File Header] --> B[BLAKE3 Root Hash] B --> C[Hash Tree Root] C --> D[Hash Branch Nodes] C --> E[Hash Branch Nodes] D --> F[Hash Leaf 1] D --> G[Hash Leaf 2] E --> H[Hash Leaf 3] E --> I[Hash Leaf 4] F --> J[Data Chunk 1<br/>CRC32 Protected] G --> K[Data Chunk 2<br/>CRC32 Protected] H --> L[Data Chunk 3<br/>CRC32 Protected] I --> M[Data Chunk 4<br/>CRC32 Protected] ``` #### 3. Performance-First Architecture ```mermaid graph LR A[Memory-Mapped File] --> B[Zero-Copy Access] B --> C[One-Time Hydration] C --> D[Pointer-Speed Runtime] E[Chunk-Based Loading] --> F[Parallel Processing] F --> G[NUMA-Aware Allocation] G --> H[SIMD Acceleration] ``` ### Technical Specifications Summary | Feature | Specification | |---------|---------------| | **File Size Support** | Up to 1 TB using C23 `_BitInt(40)` offsets | | **Load Performance** | <200ms for 1GB bundles on 8-core systems | | **Task Latency** | P99 <3ms for individual operations | | **Memory Efficiency** | Arena allocation ≤1.5× total data size | | **Integrity** | BLAKE3 Merkle trees + per-chunk CRC32 | | **Compression** | zstd with dictionary training support | | **Platforms** | Linux, macOS, Windows (x86_64, ARM64, i386) | | **Compilers** | Clang 15+, GCC 13+, MSVC 2022 17.5+ | ### Business Impact #### For Game Developers - **90% reduction** in asset loading times - **Unified pipeline** for all asset types - **Built-in hot-reload** for live development - **Cryptographic verification** for secure distribution #### For Engine Developers - **Single integration point** for all asset types - **Native graph processing** capabilities - **Zero-copy performance** characteristics - **Production-grade security** model #### For Content Creators - **Unified toolchain** across all asset types - **Visual dependency management** through graph visualization - **Automated optimization** recommendations - **Cross-platform consistency** guaranteed --- ## Architecture Overview ### System Architecture ```mermaid graph TB subgraph \"Application Layer\" A[Game Engine] B[Content Tools] C[Asset Processors] end subgraph \"GRAPHITE Core APIs\" D[graphite_core.h<br/>Loading & Access] E[graphite_tooling.h<br/>Creation & Analysis] F[graphite_unity.h<br/>Unity Integration] G[graphite_unreal.h<br/>Unreal Integration] end subgraph \"GRAPHITE Runtime\" H[Memory Manager<br/>NUMA-aware arenas] I[I/O Subsystem<br/>Async loading] J[Integrity Engine<br/>BLAKE3 verification] K[Compression Engine<br/>zstd with dictionaries] end subgraph \"Platform Layer\" L[Linux<br/>io_uring, huge pages] M[Windows<br/>Overlapped I/O, IOCP] N[macOS<br/>Unified memory, Metal] end subgraph \"Hardware Layer\" O[x86_64<br/>SIMD CRC32, prefetch] P[ARM64<br/>Hardware CRC, Apple Silicon] Q[i386<br/>Software fallbacks] end A --> D B --> E C --> E A --> F A --> G D --> H D --> I E --> J E --> K H --> L I --> M J --> N L --> O M --> P N --> Q ``` ### Data Flow Architecture ```mermaid sequenceDiagram participant App as Application participant Core as GRAPHITE Core participant IO as I/O Subsystem participant Mem as Memory Manager participant Int as Integrity Engine participant FS as File System App->>Core: graphite_open(\"bundle.graphite\") Core->>IO: mmap_file() IO->>FS: Memory map request FS-->>IO: Mapped memory region Core->>Int: verify_header() Int->>Int: Check magic, version, endianness Core->>Mem: calculate_arena_size() Mem->>Mem: Scan chunk table for totals Mem-->>Core: Arena size calculated Core->>Mem: allocate_arena() Mem->>FS: mmap arena (huge pages if available) FS-->>Mem: Arena allocated par Parallel Processing Core->>Int: verify_chunk_crcs() Core->>IO: decompress_chunks() Core->>Int: verify_hash_tree() end Core->>Mem: hydrate_pointers() Mem->>Mem: Convert offsets to pointers Core-->>App: Bundle ready for use loop Runtime Access App->>Core: graphite_get_node() Core-->>App: Direct pointer access end ``` ### Memory Architecture ```mermaid graph TB subgraph \"Virtual Memory Space\" subgraph \"File Mapping (Read-Only)\" A[File Header<br/>128 bytes] B[Chunk Table<br/>24 bytes × N] C[Data Chunks<br/>Variable size] end subgraph \"Arena (Read-Write)\" D[Node Pointer Tables<br/>8 bytes × nodes] E[Edge Pointer Tables<br/>12 bytes × edges] F[Property Tables<br/>8 bytes × props] G[String Cache<br/>Interned strings] end subgraph \"Decompression Buffers\" H[Chunk Buffer 1] I[Chunk Buffer 2] J[Chunk Buffer N] end end A --> D B --> E C --> F style A fill:#f9f9f9 style B fill:#f9f9f9 style C fill:#f9f9f9 style D fill:#e1f5fe style E fill:#e1f5fe style F fill:#e1f5fe style G fill:#e1f5fe ``` --- ## Core Principles & Design Philosophy ### 1. Universal Graph Abstraction Every element in the GRAPHITE ecosystem is modeled as a graph, providing unprecedented consistency and composability: #### Asset as Graph ```mermaid graph LR A[Asset Graph] --> B[Properties] B --> C[data_blob_id: 42] B --> D[mime_type: image/png] B --> E[size: 204800] B --> F[created: 2025-06-28] ``` #### Collection as Graph ```mermaid graph TB A[Sprite Atlas] --> B[Player Sprite] A --> C[Enemy Sprite] A --> D[UI Elements] B --> E[Walk Animation] B --> F[Idle Animation] C --> G[Attack Animation] C --> H[Death Animation] ``` #### Transform as Graph ```mermaid graph LR subgraph \"Input Subgraph\" A[main.scss] B[variables.scss] B --> A end subgraph \"Transform Logic\" C[SASS Compiler] D[Autoprefixer] E[Minifier] C --> D D --> E end subgraph \"Output Subgraph\" F[main.css] G[main.min.css] F --> G end A --> C E --> F ``` ### 2. Recursive Composition Graphs can contain other graphs without limit, enabling natural hierarchical organization: ```mermaid graph TB subgraph \"Game Bundle Graph\" subgraph \"Level 1 Assets\" A[Terrain Graph] B[Character Graph] C[Audio Graph] end subgraph \"Level 2 Components\" A --> D[Heightmap] A --> E[Texture Atlas] B --> F[Mesh Data] B --> G[Animation Rig] C --> H[Music Tracks] C --> I[Sound Effects] end subgraph \"Level 3 Elements\" E --> J[Diffuse Map] E --> K[Normal Map] G --> L[Bone Hierarchy] G --> M[Weight Maps] end end ``` ### 3. Zero-Copy Performance The architecture prioritizes performance through memory-mapped files and pointer hydration: ```mermaid graph LR A[File on Disk] --> B[Memory Mapping] B --> C[Chunk Table Scan] C --> D[Arena Allocation] D --> E[Pointer Hydration] E --> F[Runtime Ready] style A fill:#ffebee style B fill:#fff3e0 style C fill:#f3e5f5 style D fill:#e8f5e8 style E fill:#e3f2fd style F fill:#f1f8e9 ``` ### 4. Cryptographic Integrity Security is built into the format at the foundational level: ```mermaid graph TB A[File Creation] --> B[Generate BLAKE3 Tree] B --> C[Embed Root Hash in Header] C --> D[File Distribution] E[File Loading] --> F[Extract Root Hash] F --> G[Verify BLAKE3 Tree] G --> H{Hash Valid?} H -->|Yes| I[Continue Loading] H -->|No| J[Abort with Tamper Evidence] ``` --- ## Binary Format Specification ### File Structure Overview ``` ┌─────────────────────────────────┐ Offset 0x00 │ File Header 128 bytes │ ├─────────────────────────────────┤ Offset 0x80 │ Chunk Table #chunks × 24 B │ ├─────────────────────────────────┤ Variable │ Data Chunks Variable │ │ ┌─────────────────────────────┐ │ │ │ Graph Chunks │ │ │ │ Blob Chunks │ │ │ │ Hash Chunks │ │ │ │ Compressed Chunks │ │ │ └─────────────────────────────┘ │ └─────────────────────────────────┘ ``` ### File Header Specification ```c // All fields in little-endian byte order typedef struct { // Magic identification (bytes 0-3) char magic[4]; // \"GRPH\" (0x47525048) // Version and format information (bytes 4-7) uint8_t version; // 0x03 (version 3.0) uint8_t endian; // 0x00 (little-endian only) uint16_t header_size; // 0x0080 (128 bytes) // File size information (bytes 8-15) uint64_t file_size; // Total file size in bytes // Critical chunk indices (bytes 16-31) uint64_t root_graph_index; // Index of main graph chunk uint64_t string_pool_index; // Index of string pool chunk uint64_t integrity_root_index; // Index of hash tree root // Metadata (bytes 32-47) uint32_t file_flags; // File-level flags (see below) uint32_t chunk_count; // Number of chunks in table uint64_t creation_timestamp; // Unix timestamp of creation // Integrity (bytes 48-79) uint8_t file_digest[32]; // BLAKE3 hash of entire file // (excluding this field) // Reserved for future expansion (bytes 80-127) uint8_t reserved[48]; // Must be zero } graphite_file_header; ``` #### File Flags Specification | Bit | Flag Name | Description | |-----|-----------|-------------| | 0 | `MANDATORY_INTEGRITY_CHECK` | Hash verification required for loading | | 1 | `COMPRESSED_CHUNKS_PRESENT` | File contains zstd compressed chunks | | 2 | `ENCRYPTED_CHUNKS_PRESENT` | File contains AES-GCM encrypted chunks | | 3 | `DICTIONARY_COMPRESSION` | Uses compression dictionaries | | 4 | `SIGNED_BUNDLE` | Contains cryptographic signature | | 5-15 | Reserved | Must be zero | | 16-31 | Vendor Extensions | Available for implementation-specific use | ### Chunk Table Specification Each chunk table entry is exactly 24 bytes: ```c typedef struct { // Offset and size using C23 _BitInt for exact precision _BitInt(40) offset; // File offset (5 bytes, supports 1TB) _BitInt(40) size; // Chunk size (5 bytes, supports 1TB) // Type and flags (2 bytes) uint8_t kind; // Chunk type (see below) uint8_t flags; // Chunk flags (see below) // Integrity and alignment (8 bytes) uint32_t crc32; // IEEE 802.3 CRC32 of chunk data uint32_t reserved; // Padding for 8-byte alignment } chunk_table_entry; // Compile-time assertions for exact size static_assert(sizeof(chunk_table_entry) == 24, \"Chunk table entry must be exactly 24 bytes\"); ``` #### Chunk Types | Kind | Type | Structure | Description | |------|------|-----------|-------------| | 0 | `CHUNK_BLOB` | Raw binary data | Images, audio, arbitrary binary content | | 1 | `CHUNK_GRAPH` | Graph structure | Node/edge/property tables with graph header | | 2 | `CHUNK_HASH_LEAF` | Hash leaf node | Points to data chunk with BLAKE3 digest | | 3 | `CHUNK_HASH_BRANCH` | Hash branch node | Contains child hash references | | 4-15 | Reserved | Future use | Reserved for format extensions | | 16-255 | Vendor | Implementation-specific | Available for custom chunk types | #### Chunk Flags | Bit | Flag Name | Description | |-----|-----------|-------------| | 0 | `ZSTD_COMPRESSED` | Chunk data is zstd compressed | | 1 | `AES_ENCRYPTED` | Chunk data is AES-GCM encrypted | | 2 | `DICTIONARY_COMPRESSED` | Uses compression dictionary | | 3 | `INTEGRITY_CRITICAL` | Integrity failure aborts entire load | | 4-7 | Reserved | Must be zero | ### Graph Chunk Structure Graph chunks contain the core graph data structures: ```c // Graph chunk header (64 bytes, 64-byte aligned) typedef struct { // Counts (16 bytes) uint32_t node_count; // Number of child graphs uint32_t edge_count; // Number of relationships uint32_t property_count; // Number of key-value properties uint32_t graph_flags; // Graph-specific flags // Table offsets from start of chunk (32 bytes) uint64_t node_table_offset; // Offset to node index table uint64_t edge_table_offset; // Offset to edge descriptor table uint64_t property_table_offset; // Offset to property table uint64_t metadata_offset; // Offset to graph metadata // Reserved (16 bytes) uint64_t reserved1; uint64_t reserved2; } graph_chunk_header; ``` #### Graph Flags | Bit | Flag Name | Description | |-----|-----------|-------------| | 0 | `HAS_CYCLES` | Graph contains circular references | | 1 | `PARALLEL_GROUP` | Child nodes can execute concurrently | | 2 | `STRING_POOL` | Graph represents string pool | | 3 | `READONLY` | Graph should not be modified | | 4 | `SORTED_NODES` | Nodes are sorted by some criterion | | 5 | `SORTED_EDGES` | Edges are sorted by some criterion | | 6-7 | Reserved | Must be zero | | 8-15 | Optimization Hints | Implementation-specific optimization flags | | 16-31 | Vendor Extensions | Available for custom use | #### Node Index Table Array of ULEB128-encoded chunk indices: ``` Node Table Layout: ┌─────────────┬─────────────┬─────────────┬─────────────┐ │ Chunk Idx 0 │ Chunk Idx 1 │ Chunk Idx 2 │ ... │ │ (ULEB128) │ (ULEB128) │ (ULEB128) │ │ └─────────────┴─────────────┴─────────────┴─────────────┘ ``` #### Edge Descriptor Table Fixed-size edge descriptors: ```c typedef struct { uint32_t from_node_index; // Source node (index into node table) uint32_t to_node_index; // Target node (index into node table) uint32_t edge_data_chunk_index; // Chunk containing edge graph data uint32_t edge_flags; // Edge-specific flags } edge_descriptor; ``` #### Property Table Key-value pairs as ULEB128-encoded string pool indices: ``` Property Table Layout: ┌─────────────┬─────────────┬─────────────┬─────────────┬───── │ Key ID 0 │ Value ID 0 │ Key ID 1 │ Value ID 1 │ ... │ (ULEB128) │ (ULEB128) │ (ULEB128) │ (ULEB128) │ └─────────────┴─────────────┴─────────────┴─────────────┴───── ``` ### Compression Format Compressed chunks use the following structure: ```c typedef struct { uint32_t uncompressed_size; // Original size before compression uint32_t dictionary_chunk_index; // 0 if no dictionary used uint32_t compression_flags; // Compression-specific flags uint32_t checksum; // Checksum of uncompressed data uint8_t compressed_data[]; // zstd compressed payload } compressed_chunk_header; ``` --- ## Graph Model & Data Structures ### Core Graph Concepts #### Graph Definition In GRAPHITE, a graph is defined as: - **G = (V, E, P)** where: - **V** = set of vertices (nodes), each referencing another graph - **E** = set of edges (relationships), each containing a graph with relationship semantics - **P** = set of properties (key-value pairs with metadata) #### Recursive Graph Structure ```mermaid graph TB subgraph \"Level 0: Root Graph\" A[Game Bundle] end subgraph \"Level 1: Asset Category Graphs\" A --> B[Textures] A --> C[Models] A --> D[Audio] A --> E[Scripts] end subgraph \"Level 2: Asset Instance Graphs\" B --> F[player.png] B --> G[ui_atlas.png] C --> H[character.obj] C --> I[environment.fbx] end subgraph \"Level 3: Asset Component Graphs\" G --> J[button_normal] G --> K[button_hover] G --> L[button_pressed] I --> M[mesh_data] I --> N[material_refs] I --> O[animation_clips] end ``` ### Special Graph Types #### 1. Asset Graphs (Leaf Nodes) Asset graphs represent individual files or data blobs: ```c // Asset graph structure typedef struct { graph_chunk_header header; // node_count = 0, edge_count = 0 property_table properties; // Asset metadata // No node or edge tables } asset_graph; // Common asset properties static const char* ASSET_PROPERTIES[] = { \"data_blob_id\", // Chunk index of actual data \"mime_type\", // Content type (image/png, etc.) \"original_size\", // Size before compression \"checksum\", // Content checksum \"created_timestamp\", // Creation time \"modified_timestamp\", // Last modification time \"creator_tool\", // Tool that created this asset \"compression_ratio\", // Achieved compression ratio NULL }; ``` #### 2. String Pool Graphs String pools provide efficient string storage and deduplication: ```c // String pool graph typedef struct { graph_chunk_header header; // edge_count = 0, flags |= STRING_POOL node_index_table nodes; // Each node → blob chunk with UTF-8 string property_table metadata; // Pool statistics and configuration } string_pool_graph; // String pool statistics typedef struct { uint32_t total_strings; // Number of unique strings uint32_t total_bytes; // Total bytes of string data uint32_t average_length; // Average string length uint32_t deduplication_ratio; // Space saved by deduplication } string_pool_stats; ``` #### 3. Parallel Group Graphs Parallel groups indicate nodes that can be processed concurrently: ```mermaid graph TB subgraph \"Parallel Image Processing\" A[Source Image] --> B[Resize Operation] A --> C[Format Conversion] A --> D[Compression] A --> E[Thumbnail Generation] style B fill:#e8f5e8 style C fill:#e8f5e8 style D fill:#e8f5e8 style E fill:#e8f5e8 end ``` ```c // Parallel group graph typedef struct { graph_chunk_header header; // flags |= PARALLEL_GROUP node_index_table nodes; // Nodes that can execute in parallel edge_descriptor_table edges; // Dependencies between nodes property_table configuration; // Parallelization hints } parallel_group_graph; ``` #### 4. Transform Graphs Transform graphs represent complex processing pipelines: ```mermaid graph LR subgraph \"SCSS Transform Graph\" subgraph \"Input Subgraph\" A[main.scss] B[_variables.scss] C[_mixins.scss] B --> A C --> A end subgraph \"Transform Pipeline\" D[Parse SCSS] E[Resolve Imports] F[Compile CSS] G[Autoprefix] H[Minify] D --> E E --> F F --> G G --> H end subgraph \"Output Subgraph\" I[main.css] J[main.min.css] K[source.map] I --> J I --> K end A --> D H --> I end ``` ### Edge Semantics Edges in GRAPHITE are themselves graphs, allowing rich relationship modeling: #### Simple Dependency Edge ```c // Simple dependency relationship typedef struct { graph_chunk_header header; // node_count = 0, edge_count = 0 property_table properties; // Dependency metadata } dependency_edge; // Common dependency properties static const char* DEPENDENCY_PROPERTIES[] = { \"type\", // \"dependency\", \"import\", \"reference\" \"optional\", // \"true\" or \"false\" \"load_order\", // \"before\", \"after\", \"concurrent\" \"cache_policy\", // \"always\", \"conditional\", \"never\" \"version_constraint\", // Semantic version requirement NULL }; ``` #### Conditional Edge ```mermaid graph TB A[Source Asset] --> B{Condition Graph} B -->|env == \"production\"| C[Minified Output] B -->|env == \"development\"| D[Debug Output] B -->|platform == \"mobile\"| E[Optimized Output] ``` ```c // Conditional edge with logic typedef struct { graph_chunk_header header; // Contains condition evaluation graph node_index_table condition_nodes; // Condition expressions edge_descriptor_table logic_flow; // Condition evaluation flow property_table parameters; // Condition parameters } conditional_edge; ``` #### Transform Pipeline Edge ```c // Multi-step transformation edge typedef struct { graph_chunk_header header; // Complex internal structure node_index_table transform_steps; // Individual transformation steps edge_descriptor_table step_flow; // Dependencies between steps property_table configuration; // Transform parameters } transform_pipeline_edge; ``` ### Property System Properties provide flexible metadata storage: #### Property Types ```c typedef enum { PROPERTY_STRING, // String pool reference PROPERTY_INTEGER, // 64-bit signed integer PROPERTY_FLOAT, // 64-bit IEEE 754 float PROPERTY_BOOLEAN, // True/false value PROPERTY_TIMESTAMP, // Unix timestamp PROPERTY_BLOB_REFERENCE, // Reference to blob chunk PROPERTY_GRAPH_REFERENCE, // Reference to graph chunk PROPERTY_ARRAY, // Array of other properties PROPERTY_OBJECT // Nested property object } property_type; ``` #### Property Encoding ```c // Property entry in property table typedef struct { uint32_t key_string_id; // String pool ID for key uint32_t value_type; // Property type uint64_t value_data; // Type-specific value data } property_entry; // Array and object properties typedef struct { uint32_t element_count; // Number of elements/fields property_entry elements[]; // Array elements or object fields } complex_property; ``` #### Reserved Property Keys Standard property keys with semantic meaning: | Key | Type | Description | |-----|------|-------------| | `data_blob_id` | Integer | Chunk index containing asset data | | `mime_type` | String | MIME type of content | | `size` | Integer | Original uncompressed size | | `checksum` | String | Content checksum (hex encoded) | | `created` | Timestamp | Creation time | | `modified` | Timestamp | Last modification time | | `version` | String | Semantic version string | | `author` | String | Creator identification | | `license` | String | License information | | `dependencies` | Array | List of dependency specifications | | `compression_algorithm` | String | Compression method used | | `compression_ratio` | Float | Achieved compression ratio | | `load_priority` | Integer | Loading priority hint | | `cache_duration` | Integer | Cache duration in seconds | --- ## Integrity & Security System ### Overview GRAPHITE implements a multi-layered security architecture: ```mermaid graph TB A[File Level] --> B[BLAKE3 Root Hash in Header] C[Chunk Level] --> D[CRC32 per Chunk] E[Content Level] --> F[Individual Hash Leaves] G[Transport Level] --> H[Optional AES-GCM Encryption] B --> I[Tamper Detection] D --> J[Corruption Detection] F --> K[Content Verification] H --> L[Confidentiality Protection] ``` ### BLAKE3 Merkle Tree Implementation #### Hash Tree Structure ```mermaid graph TB A[Root Hash<br/>In File Header] --> B[Branch Node<br/>Chunk Index 5] A --> C[Branch Node<br/>Chunk Index 6] B --> D[Leaf Node<br/>Protects Chunk 0] B --> E[Leaf Node<br/>Protects Chunk 1] C --> F[Leaf Node<br/>Protects Chunk 2] C --> G[Leaf Node<br/>Protects Chunk 3] D --> H[Data Chunk 0<br/>Texture Data] E --> I[Data Chunk 1<br/>Mesh Data] F --> J[Data Chunk 2<br/>Audio Data] G --> K[Data Chunk 3<br/>Script Data] ``` #### Hash Leaf Structure ```c // Hash leaf chunk (protects individual data chunks) typedef struct { graph_chunk_header header; // node_count = 0, edge_count = 0 property_table properties; // Hash metadata } hash_leaf_chunk; // Hash leaf properties static const char* HASH_LEAF_PROPERTIES[] = { \"algorithm\", // \"blake3\" \"digest\", // Blob chunk ID containing 32-byte hash \"target_chunk_index\", // Index of protected chunk \"hash_timestamp\", // When hash was computed \"verification_level\", // \"required\", \"optional\", \"warning\" NULL }; ``` #### Hash Branch Structure ```c // Hash branch chunk (combines child hashes) typedef struct { graph_chunk_header header; // Contains child hash references node_index_table child_hashes; // References to child hash chunks edge_descriptor_table hash_tree; // Tree structure relationships property_table branch_metadata; // Branch-specific properties } hash_branch_chunk; ``` ### Verification Algorithm #### Complete Verification Process ```mermaid sequenceDiagram participant Loader as GRAPHITE Loader participant Header as File Header participant Tree as Hash Tree participant Chunks as Data Chunks participant Blake3 as BLAKE3 Engine Loader->>Header: Read file_digest Loader->>Tree: Load hash tree root loop For each hash branch Loader->>Tree: Get child hashes Tree-->>Loader: Child hash references end loop For each hash leaf Loader->>Chunks: Read protected chunk Chunks-->>Loader: Raw chunk data Loader->>Blake3: Hash chunk data Blake3-->>Loader: Computed hash Loader->>Tree: Compare with stored hash alt Hash matches Loader->>Loader: Continue verification else Hash mismatch Loader->>Loader: ABORT - Tamper detected end end Loader->>Blake3: Hash entire tree Blake3-->>Loader: Root hash Loader->>Header: Compare with file_digest alt Root hash matches Loader->>Loader: Verification successful else Root hash mismatch Loader->>Loader: ABORT - File corrupted end ``` #### Implementation ```c // Verification result codes typedef enum { VERIFICATION_SUCCESS, // All hashes verified VERIFICATION_CORRUPTION, // File corruption detected VERIFICATION_TAMPER, // Deliberate modification detected VERIFICATION_MISSING_HASH, // Required hash not found VERIFICATION_ALGORITHM_UNKNOWN, // Unsupported hash algorithm VERIFICATION_TIMEOUT // Verification took too long } verification_result; // Verify entire bundle integrity verification_result verify_bundle_integrity(const graphite_bundle* bundle) { // Step 1: Verify file header if (!verify_file_header(bundle)) { return VERIFICATION_CORRUPTION; } // Step 2: Verify chunk table integrity if (!verify_chunk_table(bundle)) { return VERIFICATION_CORRUPTION; } // Step 3: Verify hash tree if present if (bundle->header.integrity_root_index != 0) { verification_result result = verify_hash_tree(bundle); if (result != VERIFICATION_SUCCESS) { return result; } } // Step 4: Verify chunk CRC32s for (uint32_t i = 0; i < bundle->header.chunk_count; i++) { if (!verify_chunk_crc32(bundle, i)) { return VERIFICATION_CORRUPTION; } } return VERIFICATION_SUCCESS; } // Verify individual hash tree node bool verify_hash_node(const graphite_bundle* bundle, uint32_t hash_chunk_index) { const hash_chunk* chunk = get_hash_chunk(bundle, hash_chunk_index); if (is_hash_leaf(chunk)) { // Leaf node: verify data chunk uint32_t target_chunk = get_target_chunk_index(chunk); const void* data = get_chunk_data(bundle, target_chunk); size_t size = get_chunk_size(bundle, target_chunk); uint8_t computed_hash[32]; blake3(data, size, computed_hash); const uint8_t* stored_hash = get_stored_hash(chunk); return memcmp(computed_hash, stored_hash, 32) == 0; } else { // Branch node: verify all children, then compute branch hash for (uint32_t i = 0; i < chunk->header.node_count; i++) { uint32_t child_index = get_child_hash_index(chunk, i); if (!verify_hash_node(bundle, child_index)) { return false; } } // Compute hash of concatenated child hashes uint8_t* child_hashes = collect_child_hashes(chunk); size_t total_size = chunk->header.node_count * 32; uint8_t computed_hash[32]; blake3(child_hashes, total_size, computed_hash); const uint8_t* stored_hash = get_stored_hash(chunk); return memcmp(computed_hash, stored_hash, 32) == 0; } } ``` ### Encryption Support #### AES-GCM Chunk Encryption ```c // Encrypted chunk header typedef struct { uint32_t encryption_algorithm; // AES_GCM_256 uint32_t key_derivation_method; // PBKDF2, SCRYPT, etc. uint8_t initialization_vector[16]; // Random IV for this chunk uint8_t authentication_tag[16]; // GCM authentication tag uint32_t encrypted_size; // Size of encrypted payload uint8_t encrypted_data[]; // AES-GCM encrypted content } encrypted_chunk_header; // Encryption context typedef struct { uint8_t master_key[32]; // AES-256 master key uint32_t key_iterations; // PBKDF2 iteration count uint8_t salt[32]; // Random salt for key derivation } encryption_context; ``` ### Security Threat Mitigation #### File Integrity Attacks | Attack Vector | Mitigation | Implementation | |---------------|------------|----------------| | **Malicious Modification** | BLAKE3 Merkle Tree | Root hash in file header, recursive verification | | **Bit Flips / Corruption** | CRC32 per chunk | Hardware CRC32 when available | | **Replay Attacks** | Hash Root Binding | Root hash cryptographically bound to file | | **Partial Modification** | Incremental Verification | Verify only accessed chunks if needed | #### Memory Safety Attacks | Attack Vector | Mitigation | Implementation | |---------------|------------|----------------| | **Buffer Overflows** | Bounds Checking | All offsets validated against file size | | **Integer Overflows** | Safe Arithmetic | Checked arithmetic for all size calculations | | **Format String** | No Dynamic Formats | All format strings are compile-time constants | | **Heap Corruption** | Arena Allocation | Isolated allocation patterns | #### Denial of Service Attacks | Attack Vector | Mitigation | Implementation | |---------------|------------|----------------| | **Decompression Bombs** | Size Limits | Max expansion ratio limits | | **Hash Collision** | BLAKE3 | Cryptographically secure hash function | | **Resource Exhaustion** | Quotas | Arena size limits, timeout protection | | **Recursive Bombs** | Depth Limits | Maximum graph nesting depth | --- ## Compression & Optimization ### Compression Architecture ```mermaid graph TB subgraph \"Compression Decision Tree\" A[Chunk Analysis] --> B{Size > 64KB?} B -->|No| C[No Compression] B -->|Yes| D{Content Type} D -->|Text/JSON/Script| E[zstd Level 3] D -->|Binary/Media| F{Size > 1MB?} F -->|No| G[zstd Level 5] F -->|Yes| H{Update Frequency} H -->|Rare| I[zstd Level 9] H -->|Frequent| J[zstd Level 5] end ``` ### zstd Integration #### Compression Level Selection ```c // Compression level decision matrix typedef struct { size_t chunk_size; content_type type; update_frequency frequency; int recommended_level; double target_ratio; } compression_config; static const compression_config COMPRESSION_MATRIX[] = { // Size, Type, Frequency, Level, Ratio {64*1024, CONTENT_ANY, FREQ_ANY, 0, 1.0}, // No compression {1*1024*1024, CONTENT_TEXT, FREQ_ANY, 3, 0.6}, // Fast text compression {1*1024*1024, CONTENT_BINARY, FREQ_HIGH, 5, 0.8}, // Balanced binary {50*1024*1024, CONTENT_ANY, FREQ_LOW, 9, 0.5}, // Maximum compression }; int select_compression_level(size_t size, content_type type, update_frequency freq) { for (const compression_config* config = COMPRESSION_MATRIX; config->chunk_size != 0; config++) { if (size <= config->chunk_size && (config->type == CONTENT_ANY || config->type == type) && (config->frequency == FREQ_ANY || config->frequency == freq)) { return config->recommended_level; } } return 5; // Default level } ``` #### Dictionary Training Dictionary training provides significant compression improvements for similar content: ```mermaid graph LR A[Collect Training Samples] --> B[Analyze Content Patterns] B --> C[Generate Dictionary] C --> D[Store Dictionary as Chunk] D --> E[Reference in Compressed Chunks] F[Sample 1: config.json] --> A G[Sample 2: metadata.json] --> A H[Sample 3: manifest.json] --> A I[Sample N: data.json] --> A ``` ```c // Dictionary training process typedef struct { void** samples; // Array of sample data pointers size_t* sample_sizes; // Size of each sample size_t sample_count; // Number of samples size_t max_dict_size; // Maximum dictionary size } training_set; // Train compression dictionary uint8_t* train_compression_dictionary(const training_set* training) { // Collect all training data size_t total_size = 0; for (size_t i = 0; i < training->sample_count; i++) { total_size += training->sample_sizes[i]; } uint8_t* combined_data = malloc(total_size); size_t offset = 0; for (size_t i = 0; i < training->sample_count; i++) { memcpy(combined_data + offset, training->samples[i], training->sample_sizes[i]); offset += training->sample_sizes[i]; } // Use zstd dictionary training size_t dict_size = ZDICT_trainFromBuffer( dict_buffer, training->max_dict_size, combined_data, total_size ); free(combined_data); if (ZDICT_isError(dict_size)) { return NULL; } return dict_buffer; } ``` #### Compression Format ```c // Compressed chunk layout typedef struct { // Header (16 bytes) uint32_t original_size; // Size before compression uint32_t compression_algorithm; // ZSTD = 1 uint32_t dictionary_chunk_index; // 0 if no dictionary uint32_t compression_flags; // Algorithm-specific flags // zstd compressed data follows immediately uint8_t compressed_payload[]; } compressed_chunk; // Decompression process bool decompress_chunk(const graphite_bundle* bundle, uint32_t chunk_index, void* output_buffer, size_t output_size) { const compressed_chunk* chunk = get_compressed_chunk(bundle, chunk_index); // Load dictionary if specified ZSTD_DDict* dict = NULL; if (chunk->dictionary_chunk_index != 0) { const void* dict_data = get_chunk_data(bundle, chunk->dictionary_chunk_index); size_t dict_size = get_chunk_size(bundle, chunk->dictionary_chunk_index); dict = ZSTD_createDDict(dict_data, dict_size); } // Decompress data size_t result; if (dict) { result = ZSTD_decompress_usingDDict( output_buffer, output_size, chunk->compressed_payload, get_compressed_payload_size(chunk), dict ); ZSTD_freeDDict(dict); } else { result = ZSTD_decompress( output_buffer, output_size, chunk->compressed_payload, get_compressed_payload_size(chunk) ); } return !ZSTD_isError(result) && result == chunk->original_size; } ``` ### Content-Aware Optimization #### File Type Detection ```c // Content type detection for optimization typedef enum { CONTENT_UNKNOWN, CONTENT_TEXT_PLAIN, CONTENT_TEXT_JSON, CONTENT_TEXT_XML, CONTENT_TEXT_SCRIPT, CONTENT_IMAGE_PNG, CONTENT_IMAGE_JPEG, CONTENT_IMAGE_DDS, CONTENT_AUDIO_WAV, CONTENT_AUDIO_OGG, CONTENT_MESH_OBJ, CONTENT_MESH_FBX, CONTENT_BINARY_UNKNOWN } content_type; content_type detect_content_type(const void* data, size_t size, const char* filename_hint) { // Magic number detection const uint8_t* bytes = (const uint8_t*)data; if (size >= 8 && memcmp(bytes, \"\\x89PNG\\r\ \\x1a\ \", 8) == 0) { return CONTENT_IMAGE_PNG; } if (size >= 3 && memcmp(bytes, \"\\xFF\\xD8\\xFF\", 3) == 0) { return CONTENT_IMAGE_JPEG; } if (size >= 4 && memcmp(bytes, \"DDS \", 4) == 0) { return CONTENT_IMAGE_DDS; } if (size >= 4 && memcmp(bytes, \"RIFF\", 4) == 0) { return CONTENT_AUDIO_WAV; } if (size >= 4 && memcmp(bytes, \"OggS\", 4) == 0) { return CONTENT_AUDIO_OGG; } // Content analysis for text formats if (is_likely_text(data, min(size, 1024))) { if (filename_hint) { if (strstr(filename_hint, \".json\")) return CONTENT_TEXT_JSON; if (strstr(filename_hint, \".xml\")) return CONTENT_TEXT_XML; if (strstr(filename_hint, \".js\") || strstr(filename_hint, \".py\")) { return CONTENT_TEXT_SCRIPT; } } // JSON detection if (bytes[0] == '{' || bytes[0] == '[') { return CONTENT_TEXT_JSON; } // XML detection if (memcmp(bytes, \"<?xml\", 5) == 0 || bytes[0] == '<') { return CONTENT_TEXT_XML; } return CONTENT_TEXT_PLAIN; } return CONTENT_BINARY_UNKNOWN; } ``` #### Compression Effectiveness Analysis ```c // Analyze compression effectiveness typedef struct { double compression_ratio; // Compressed size / original size uint64_t compression_time_ns; // Time to compress uint64_t decompression_time_ns; // Time to decompress size_t memory_usage; // Peak memory during compression quality_score quality; // Lossy compression quality } compression_analysis; compression_analysis analyze_compression(const void* data, size_t size, int compression_level) { compression_analysis result = {0}; auto start_time = high_resolution_clock::now(); // Perform compression size_t compressed_bound = ZSTD_compressBound(size); void* compressed_buffer = malloc(compressed_bound); size_t compressed_size = ZSTD_compress( compressed_buffer, compressed_bound, data, size, compression_level ); auto compress_end = high_resolution_clock::now(); result.compression_time_ns = duration_cast<nanoseconds>( compress_end - start_time).count(); if (!ZSTD_isError(compressed_size)) { result.compression_ratio = (double)compressed_size / size; // Test decompression void* decompressed_buffer = malloc(size); auto decomp_start = high_resolution_clock::now(); size_t decompressed_size = ZSTD_decompress( decompressed_buffer, size, compressed_buffer, compressed_size ); auto decomp_end = high_resolution_clock::now(); result.decompression_time_ns = duration_cast<nanoseconds>( decomp_end - decomp_start).count(); // Verify decompression correctness if (decompressed_size == size && memcmp(data, decompressed_buffer, size) == 0) { result.quality = QUALITY_LOSSLESS; } else { result.quality = QUALITY_ERROR; } free(decompressed_buffer); } else { result.compression_ratio = 1.0; // No compression achieved result.quality = QUALITY_ERROR; } free(compressed_buffer); return result; } ``` --- ## Performance Engineering ### Architecture for Performance GRAPHITE is designed from the ground up for maximum performance: ```mermaid graph TB subgraph \"Memory Hierarchy Optimization\" A[L1 Cache<br/>Direct pointer access] --> B[L2 Cache<br/>Smart prefetching] B --> C[L3 Cache<br/>NUMA-aware allocation] C --> D[Main Memory<br/>Huge pages] D --> E[Storage<br/>Async I/O] end subgraph \"Parallelization Strategy\" F[File I/O<br/>io_uring/Overlapped] --> G[Decompression<br/>Per-chunk threads] G --> H[Verification<br/>Parallel hash tree] H --> I[Hydration<br/>Parallel pointer conversion] end subgraph \"Hardware Acceleration\" J[SIMD CRC32<br/>SSE4.2/ARM64] --> K[Hardware Prefetch<br/>Platform-specific hints] K --> L[Vector Instructions<br/>BLAKE3 optimization] end ``` ### Memory Management #### Arena Allocation Strategy GRAPHITE uses arena allocation for predictable memory usage and optimal cache performance: ```c // Arena size calculation typedef struct { size_t node_pointers; // 8 bytes × total_nodes size_t edge_descriptors; // 12 bytes × total_edges size_t property_data; // 8 bytes × total_properties size_t string_cache; // Estimated string overhead size_t alignment_padding; // 64-byte alignment requirements size_t safety_buffer; // 128KB safety margin size_t total_arena_size; // Sum of all above } arena_calculation; arena_calculation calculate_arena_requirements(const graphite_bundle* bundle) { arena_calculation calc = {0}; // Scan all graph chunks to count totals for (uint32_t i = 0; i < bundle->header.chunk_count; i++) { const chunk_table_entry* entry = get_chunk_entry(bundle, i); if (entry->kind == CHUNK_GRAPH) { const graph_chunk_header* graph = get_graph_header(bundle, i); calc.node_pointers += graph->node_count * sizeof(void*); calc.edge_descriptors += graph->edge_count * sizeof(edge_descriptor); calc.property_data += graph->property_count * 2 * sizeof(uint32_t); } } // Estimate string cache requirements calc.string_cache = estimate_string_cache_size(bundle); // Add alignment and safety margins calc.alignment_padding = 64 * 1024; // 64KB for alignment calc.safety_buffer = 128 * 1024; // 128KB safety margin calc.total_arena_size = calc.node_pointers + calc.edge_descriptors + calc.property_data + calc.string_cache + calc.alignment_padding + calc.safety_buffer; // Round up to next page boundary size_t page_size = get_page_size(); calc.total_arena_size = ((calc.total_arena_size + page_size - 1) / page_size) * page_size; return calc; } // Allocate performance-optimized arena void* allocate_optimized_arena(size_t size) { void* arena; #ifdef __linux__ // Use huge pages on Linux if available arena = mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0); if (arena != MAP_FAILED && size >= 2 * 1024 * 1024) { // Request transparent huge pages for large allocations madvise(arena, size, MADV_HUGEPAGE); // Hint that access will be sequential during hydration madvise(arena, size, MADV_SEQUENTIAL); } #elif defined(_WIN32) // Use large pages on Windows if available arena = VirtualAlloc(NULL, size, MEM_COMMIT | MEM_RESERVE, PAGE_READWRITE); if (arena && size >= 2 * 1024 * 1024) { // Try to use large pages VirtualFree(arena, 0, MEM_RELEASE); arena = VirtualAlloc(NULL, size, MEM_COMMIT | MEM_RESERVE | MEM_LARGE_PAGES, PAGE_READWRITE); if (!arena) { // Fall back to regular pages arena = VirtualAlloc(NULL, size, MEM_COMMIT | MEM_RESERVE, PAGE_READWRITE); } } #else // Standard allocation for other platforms arena = mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0); #endif return arena; } ``` #### NUMA Optimization ```c // NUMA-aware memory allocation typedef struct { int node_count; // Number of NUMA nodes int current_node; // Current thread's NUMA node cpu_set_t* node_cpus; // CPUs for each NUMA node void** arenas; // Per-node arena allocations } numa_context; #ifdef HAVE_NUMA bool setup_numa_optimizations(numa_context* ctx) { ctx->node_count = numa_num_configured_nodes(); ctx->current_node = numa_node_of_cpu(sched_getcpu()); if (ctx->node_count <= 1) { return false; // No NUMA benefit } ctx->node_cpus = malloc(ctx->node_count * sizeof(cpu_set_t)); ctx->arenas = malloc(ctx->node_count * sizeof(void*)); // Get CPU sets for each NUMA node for (int node = 0; node < ctx->node_count; node++) { CPU_ZERO(&ctx->node_cpus[node]); struct bitmask* node_mask = numa_allocate_cpumask(); numa_node_to_cpus(node, node_mask); for (int cpu = 0; cpu < numa_num_possible_cpus(); cpu++) { if (numa_bitmask_isbitset(node_mask, cpu)) { CPU_SET(cpu, &ctx->node_cpus[node]); } } numa_free_cpumask(node_mask); } return true; } void* allocate_numa_arena(numa_context* ctx, size_t size) { if (!ctx || ctx->node_count <= 1) { return allocate_optimized_arena(size); } // Allocate on current NUMA node void* arena = numa_alloc_onnode(size, ctx->current_node); if (!arena) { // Fall back to any node arena = numa_alloc(size); } return arena; } void bind_thread_to_numa_node(numa_context* ctx, int node) { if (!ctx || node >= ctx->node_count) { return; } // Set CPU affinity to this NUMA node pthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), &ctx->node_cpus[node]); } #endif ``` ### Asynchronous I/O #### Linux io_uring Implementation ```c #ifdef HAVE_IO_URING typedef struct { struct io_uring ring; // io_uring instance struct iovec* iovecs; // I/O vectors for each chunk uint32_t queue_depth; // Maximum concurrent operations uint32_t pending_ops; // Currently pending operations } async_io_context; bool setup_async_io(async_io_context* ctx, uint32_t queue_depth) { ctx->queue_depth = queue_depth; ctx->pending_ops = 0; // Initialize io_uring int ret = io_uring_queue_init(queue_depth, &ctx->ring, 0); if (ret < 0) { return false; } // Allocate I/O vectors ctx->iovecs = malloc(queue_depth * sizeof(struct iovec)); if (!ctx->iovecs) { io_uring_queue_exit(&ctx->ring); return false; } return true; } void queue_chunk_read(async_io_context* ctx, int fd, uint32_t chunk_index, void* buffer, size_t size, off_t offset) { if (ctx->pending_ops >= ctx->queue_depth) { // Wait for completion before queuing more wait_for_completion(ctx, 1); } struct io_uring_sqe* sqe = io_uring_get_sqe(&ctx->ring); ctx->iovecs[ctx->pending_ops].iov_base = buffer; ctx->iovecs[ctx->pending_ops].iov_len = size; io_uring_prep_readv(sqe, fd, &ctx->iovecs[ctx->pending_ops], 1, offset); sqe->user_data = chunk_index; ctx->pending_ops++; } void submit_and_wait_all(async_io_context* ctx) { // Submit all queued operations io_uring_submit(&ctx->ring); // Wait for all completions while (ctx->pending_ops > 0) { struct io_uring_cqe* cqe; int ret = io_uring_wait_cqe(&ctx->ring, &cqe); if (ret == 0) { uint32_t chunk_index = cqe->user_data; ssize_t bytes_read = cqe->res; if (bytes_read > 0) { process_loaded_chunk(chunk_index, bytes_read); } else { handle_io_error(chunk_index, bytes_read); } io_uring_cqe_seen(&ctx->ring, cqe); ctx->pending_ops--; } } } #endif ``` #### Windows Overlapped I/O Implementation ```c #ifdef _WIN32 typedef struct { HANDLE file_handle; // File handle HANDLE completion_port; // I/O completion port OVERLAPPED* overlappeds; // Overlapped structures void** buffers; // Read buffers uint32_t max_concurrent; // Maximum concurrent operations uint32_t pending_ops; // Currently pending operations } windows_async_io; bool setup_windows_async_io(windows_async_io* ctx, const char* filename, uint32_t max_concurrent) { ctx->max_concurrent = max_concurrent; ctx->pending_ops = 0; // Open file with overlapped flag ctx->file_handle = CreateFileA( filename, GENERIC_READ, FILE_SHARE_READ, NULL, OPEN_EXISTING, FILE_FLAG_OVERLAPPED | FILE_FLAG_NO_BUFFERING, NULL ); if (ctx->file_handle == INVALID_HANDLE_VALUE) { return false; } // Create I/O completion port ctx->completion_port = CreateIoCompletionPort( ctx->file_handle, NULL, 0, 0 ); if (!ctx->completion_port) { CloseHandle(ctx->file_handle); return false; } // Allocate overlapped structures and buffers ctx->overlappeds = malloc(max_concurrent * sizeof(OVERLAPPED)); ctx->buffers = malloc(max_concurrent * sizeof(void*)); return true; } void queue_windows_read(windows_async_io* ctx, uint32_t chunk_index, size_t size, uint64_t offset) { if (ctx->pending_ops >= ctx->max_concurrent) { wait_for_windows_completion(ctx, 1); } uint32_t op_index = ctx->pending_ops; // Allocate aligned buffer ctx->buffers[op_index] = _aligned_malloc(size, 4096); // Setup overlapped structure ZeroMemory(&ctx->overlappeds[op_index], sizeof(OVERLAPPED)); ctx->overlappeds[op_index].Offset = (DWORD)(offset & 0xFFFFFFFF); ctx->overlappeds[op_index].OffsetHigh = (DWORD)(offset >> 32); ctx->overlappeds[op_index].hEvent = (HANDLE)(uintptr_t)chunk_index; // Queue read operation BOOL result = ReadFile( ctx->file_handle, ctx->buffers[op_index], (DWORD)size, NULL, &ctx->overlappeds[op_index] ); if (!result && GetLastError() != ERROR_IO_PENDING) { handle_windows_io_error(chunk_index); _aligned_free(ctx->buffers[op_index]); } else { ctx->pending_ops++; } } #endif ``` ### SIMD Acceleration #### Hardware CRC32 Implementation ```c // Hardware-accelerated CRC32 for x86_64 #if defined(__x86_64__) && defined(__SSE4_2__) uint32_t crc32_hw_x86(const void* data, size_t length) { const uint8_t* ptr = (const uint8_t*)data; uint32_t crc = 0xFFFFFFFF; // Process 8 bytes at a time while (length >= 8) { uint64_t chunk = *(const uint64_t*)ptr; crc = _mm_crc32_u64(crc, chunk); ptr += 8; length -= 8; } // Process 4 bytes if (length >= 4) { uint32_t chunk = *(const uint32_t*)ptr; crc = _mm_crc32_u32(crc, chunk); ptr += 4; length -= 4; } // Process remaining bytes while (length > 0) { crc = _mm_crc32_u8(crc, *ptr); ptr++; length--; } return ~crc; } // Hardware-accelerated CRC32 for ARM64 #elif defined(__aarch64__) uint32_t crc32_hw_arm64(const void* data, size_t length) { const uint8_t* ptr = (const uint8_t*)data; uint32_t crc = 0xFFFFFFFF; // Process 8 bytes at a time while (length >= 8) { uint64_t chunk = *(const uint64_t*)ptr; crc = __crc32cd(crc, chunk); ptr += 8; length -= 8; } // Process 4 bytes if (length >= 4) { uint32_t chunk = *(const uint32_t*)ptr; crc = __crc32cw(crc, chunk); ptr += 4; length -= 4; } // Process remaining bytes while (length > 0) { crc = __crc32cb(crc, *ptr); ptr++; length--; } return ~crc; } #endif // Unified CRC32 function with hardware detection uint32_t graphite_crc32(const void* data, size_t length) { #if defined(__x86_64__) && defined(__SSE4_2__) static int hw_support = -1; if (hw_support == -1) { hw_support = __builtin_cpu_supports(\"sse4.2\"); } if (hw_support) { return crc32_hw_x86(data, length); } #elif defined(__aarch64__) return crc32_hw_arm64(data, length); #endif // Software fallback return crc32_software(data, length); } ``` #### Smart Prefetching ```c // Platform-specific prefetch optimization typedef struct { size_t l1_cache_size; // L1 data cache size size_t l2_cache_size; // L2 cache size size_t cache_line_size; // Cache line size int prefetch_distance; // Optimal prefetch distance } cache_info; cache_info get_cache_info(void) { cache_info info = {0}; #ifdef __linux__ // Linux: Read from /proc/cpuinfo or sysconf info.l1_cache_size = sysconf(_SC_LEVEL1_DCACHE_SIZE); info.l2_cache_size = sysconf(_SC_LEVEL2_CACHE_SIZE); info.cache_line_size = sysconf(_SC_LEVEL1_DCACHE_LINESIZE); #elif defined(_WIN32) // Windows: Use GetLogicalProcessorInformation SYSTEM_LOGICAL_PROCESSOR_INFORMATION* buffer = NULL; DWORD buffer_size = 0; GetLogicalProcessorInformation(NULL, &buffer_size); buffer = malloc(buffer_size); if (GetLogicalProcessorInformation(buffer, &buffer_size)) { for (DWORD i = 0; i < buffer_size / sizeof(*buffer); i++) { if (buffer[i].Relationship == RelationCache) { CACHE_DESCRIPTOR* cache = &buffer[i].Cache; if (cache->Level == 1 && cache->Type == CacheData) { info.l1_cache_size = cache->Size; info.cache_line_size = cache->LineSize; } else if (cache->Level == 2) { info.l2_cache_size = cache->Size; } } } } free(buffer); #elif defined(__APPLE__) // macOS: Use sysctlbyname size_t size = sizeof(info.l1_cache_size); sysctlbyname(\"hw.l1dcachesize\", &info.l1_cache_size, &size, NULL, 0); size = sizeof(info.l2_cache_size); sysctlbyname(\"hw.l2cachesize\", &info.l2_cache_size, &size, NULL, 0); size = sizeof(info.cache_line_size); sysctlbyname(\"hw.cachelinesize\", &info.cache_line_size, &size, NULL, 0); #endif // Set defaults if detection failed if (info.l1_cache_size == 0) info.l1_cache_size = 32 * 1024; if (info.l2_cache_size == 0) info.l2_cache_size = 256 * 1024; if (info.cache_line_size == 0) info.cache_line_size = 64; // Calculate optimal prefetch distance info.prefetch_distance = info.l2_cache_size / (64 * 1024); // Heuristic info.prefetch_distance = max(2, min(info.prefetch_distance, 8)); return info; } void prefetch_chunks_optimized(const graphite_bundle* bundle) { static cache_info cache = {0}; if (cache.l1_cache_size == 0) { cache = get_cache_info(); } // Calculate prefetch stride based on chunk sizes double avg_chunk_size = (double)bundle->total_file_size / bundle->header.chunk_count; size_t prefetch_stride = max(2, (size_t)(cache.l2_cache_size / avg_chunk_size)); prefetch_stride = min(prefetch_stride, 8); for (uint32_t i = 0; i < bundle->header.chunk_count; i++) { // Prefetch future chunks for (size_t j = 1; j <= prefetch_stride && i + j < bundle->header.chunk_count; j++) { const void* chunk_data = get_chunk_data(bundle, i + j); #if defined(__x86_64__) // x86_64: Use moderate temporal locality __builtin_prefetch(chunk_data, 0, 2); #elif defined(__aarch64__) // ARM64: Use temporal locality hint __builtin_prefetch(chunk_data, 0, 3); #else // Generic prefetch __builtin_prefetch(chunk_data, 0, 1); #endif } // Process current chunk process_chunk_data(bundle, i); } } ``` ### Multi-Threading Architecture #### Stage-Based Parallel Pipeline ```mermaid sequenceDiagram participant Main as Main Thread participant CRC as CRC Pool participant Decomp as Decompression Pool participant Hash as Hash Verification Pool participant Hydrate as Hydration Pool Main->>CRC: Dispatch CRC verification tasks par CRC Verification CRC->>CRC: Verify chunk 0 CRC32 CRC->>CRC: Verify chunk 1 CRC32 CRC->>CRC: Verify chunk N CRC32 end CRC-->>Main: Stage 1 complete Main->>Decomp: Dispatch decompression tasks par Decompression Decomp->>Decomp: Decompress chunk A Decomp->>Decomp: Decompress chunk B Decomp->>Decomp: Decompress chunk C end Decomp-->>Main: Stage 2 complete Main->>Hash: Dispatch hash verification par Hash Verification Hash->>Hash: Verify hash subtree 1 Hash->>Hash: Verify hash subtree 2 end Hash-->>Main: Stage 3 complete Main->>Hydrate: Dispatch pointer hydration par Hydration Hydrate->>Hydrate: Hydrate graph 1 Hydrate->>Hydrate: Hydrate graph 2 Hydrate->>Hydrate: Hydrate graph 3 end Hydrate-->>Main: Loading complete ``` ```c // Thread pool implementation typedef struct { pthread_t* threads; // Worker threads int thread_count; // Number of threads task_queue queue; // Work queue pthread_mutex_t queue_mutex; // Queue synchronization pthread_cond_t queue_cond; // Queue condition variable volatile bool shutdown; // Shutdown flag barrier_t stage_barrier; // Stage synchronization barrier } thread_pool; // Task types for different stages typedef enum { TASK_CRC_VERIFY, TASK_DECOMPRESS, TASK_HASH_VERIFY, TASK_HYDRATE_GRAPH } task_type; typedef struct { task_type type; uint32_t chunk_index; void* context; void (*callback)(struct task*, void* result); } task; // Stage-based parallel loading void load_bundle_parallel(graphite_bundle* bundle) { thread_pool* pool = create_thread_pool(get_cpu_count()); // Stage 1: CRC verification for (uint32_t i = 0; i < bundle->header.chunk_count; i++) { task* crc_task = create_task(TASK_CRC_VERIFY, i, bundle, NULL); queue_task(pool, crc_task); } wait_stage_completion(pool); // Stage 2: Decompression (only compressed chunks) for (uint32_t i = 0; i < bundle->header.chunk_count; i++) { if (is_compressed_chunk(bundle, i)) { task* decomp_task = create_task(TASK_DECOMPRESS, i, bundle, NULL); queue_task(pool, decomp_task); } } wait_stage_completion(pool); // Stage 3: Hash verification (parallel subtrees) if (bundle->header.integrity_root_index != 0) { hash_subtree* subtrees = find_hash_subtrees(bundle); for (int i = 0; i < subtrees->count; i++) { task* hash_task = create_task(TASK_HASH_VERIFY, subtrees->roots[i], bundle, NULL); queue_task(pool, hash_task); } wait_stage_completion(pool); } // Stage 4: Pointer hydration (parallel per top-level graph) graph_list* top_level = find_top_level_graphs(bundle); for (int i = 0; i < top_level->count; i++) { task* hydrate_task = create_task(TASK_HYDRATE_GRAPH, top_level->indices[i], bundle, NULL); queue_task(pool, hydrate_task); } wait_stage_completion(pool); destroy_thread_pool(pool); } ``` --- ## Core API Specification ### Primary Interface (graphite_core.h) The core API provides the essential functionality for loading and accessing GRAPHITE bundles: ```c #ifndef GRAPHITE_CORE_H #define GRAPHITE_CORE_H #include <stdint.h> #include <stdbool.h> #include <stddef.h> #ifdef __cplusplus extern \"C\" { #endif // Version information #define GRAPHITE_VERSION_MAJOR 3 #define GRAPHITE_VERSION_MINOR 0 #define GRAPHITE_VERSION_PATCH 0 #define GRAPHITE_VERSION_STRING \"3.0.0\" // Forward declarations - opaque handles typedef struct graphite_bundle graphite_bundle; typedef struct graphite_graph graphite_graph; typedef struct graphite_node_iterator graphite_node_iterator; typedef struct graphite_edge_iterator graphite_edge_iterator; // Load flags for bundle opening typedef enum { GRAPHITE_LOAD_DEFAULT = 0, GRAPHITE_VERIFY_HASHES = 1 << 0, // Enable cryptographic verification GRAPHITE_DECOMPRESS = 1 << 1, // Enable zstd decompression GRAPHITE_PREFETCH = 1 << 2, // Enable smart memory prefetching GRAPHITE_NUMA_AWARE = 1 << 3, // Enable NUMA optimizations GRAPHITE_PARALLEL_CRC = 1 << 4, // Parallel CRC verification GRAPHITE_HW_ACCELERATION = 1 << 5, // Use hardware acceleration GRAPHITE_VALIDATE_FORMAT = 1 << 6, // Extra format validation GRAPHITE_MLOCK_ARENA = 1 << 7, // Lock arena pages in memory } graphite_load_flags; // Error codes typedef enum { GRAPHITE_OK = 0, // File I/O errors (1-99) GRAPHITE_ERROR_FILE_NOT_FOUND = 1, GRAPHITE_ERROR_ACCESS_DENIED = 2, GRAPHITE_ERROR_DISK_FULL = 3, GRAPHITE_ERROR_IO_ERROR = 4, GRAPHITE_ERROR_FILE_LOCKED = 5, // Format errors (100-199) GRAPHITE_ERROR_INVALID_MAGIC = 100, GRAPHITE_ERROR_UNSUPPORTED_VERSION = 101, GRAPHITE_ERROR_CORRUPTED_HEADER = 102, GRAPHITE_ERROR_INVALID_CHUNK_TABLE = 103, GRAPHITE_ERROR_MALFORMED_GRAPH = 104, GRAPHITE_ERROR_INVALID_OFFSET = 105, GRAPHITE_ERROR_INVALID_SIZE = 106, // Integrity errors (200-299) GRAPHITE_ERROR_CRC_MISMATCH = 200, GRAPHITE_ERROR_HASH_VERIFICATION_FAILED = 201, GRAPHITE_ERROR_SIGNATURE_INVALID = 202, GRAPHITE_ERROR_TAMPER_DETECTED = 203, GRAPHITE_ERROR_MISSING_HASH_TREE = 204, // Resource errors (300-399) GRAPHITE_ERROR_OUT_OF_MEMORY = 300, GRAPHITE_ERROR_ARENA_EXHAUSTED = 301, GRAPHITE_ERROR_TOO_MANY_CHUNKS = 302, GRAPHITE_ERROR_FILE_TOO_LARGE = 303, GRAPHITE_ERROR_THREAD_CREATION_FAILED = 304, // Compression errors (400-499) GRAPHITE_ERROR_DECOMPRESSION_FAILED = 400, GRAPHITE_ERROR_DICTIONARY_MISSING = 401, GRAPHITE_ERROR_COMPRESSION_CORRUPTED = 402, GRAPHITE_ERROR_UNSUPPORTED_COMPRESSION = 403, // Platform errors (500-599) GRAPHITE_ERROR_NUMA_UNAVAILABLE = 500, GRAPHITE_ERROR_HUGE_PAGES_UNAVAILABLE = 501, GRAPHITE_ERROR_HARDWARE_ACCELERATION_UNAVAILABLE = 502, GRAPHITE_ERROR_MMAP_FAILED = 503 } graphite_error; // Performance statistics typedef struct { uint64_t total_load_time_ns; // Total loading time uint64_t crc_verification_time_ns; // CRC verification time uint64_t decompression_time_ns; // Decompression time uint64_t hash_verification_time_ns; // Hash verification time uint64_t hydration_time_ns; // Pointer hydration time uint64_t total_bytes_processed; // Total bytes processed uint32_t chunks_processed; // Number of chunks processed uint32_t chunks_compressed; // Number of compressed chunks uint32_t chunks_verified; // Number of hash-verified chunks size_t peak_memory_usage; // Peak memory usage during load size_t arena_size; // Arena allocation size double compression_ratio; // Overall compression ratio } graphite_performance_stats; // Bundle information typedef struct { uint32_t version; // Bundle format version uint64_t file_size; // Total file size uint32_t chunk_count; // Number of chunks uint32_t graph_count; // Number of graph chunks uint32_t blob_count; // Number of blob chunks uint64_t creation_timestamp; // Bundle creation time bool has_integrity_tree; // Has cryptographic verification bool has_compression; // Has compressed chunks bool has_encryption; // Has encrypted chunks size_t total_uncompressed_size; // Size before compression double overall_compression_ratio; // Compression effectiveness } graphite_bundle_info; // Property types typedef enum { GRAPHITE_PROPERTY_STRING, // String value (string pool reference) GRAPHITE_PROPERTY_INTEGER, // 64-bit signed integer GRAPHITE_PROPERTY_FLOAT, // 64-bit IEEE 754 float GRAPHITE_PROPERTY_BOOLEAN, // Boolean value GRAPHITE_PROPERTY_TIMESTAMP, // Unix timestamp (uint64_t) GRAPHITE_PROPERTY_BLOB_REF, // Reference to blob chunk GRAPHITE_PROPERTY_GRAPH_REF, // Reference to graph chunk GRAPHITE_PROPERTY_ARRAY, // Array of properties GRAPHITE_PROPERTY_OBJECT // Nested property object } graphite_property_type; // Property value (variant type) typedef struct { graphite_property_type type; union { const char* string_value; // Null-terminated string int64_t integer_value; // Integer value double float_value; // Floating point value bool boolean_value; // Boolean value uint64_t timestamp_value; // Unix timestamp uint32_t reference_value; // Chunk reference struct { const struct graphite_property* elements; uint32_t count; } array_value; // Array elements struct { const struct graphite_property* fields; uint32_t count; } object_value; // Object fields }; } graphite_property_value; // Property key-value pair typedef struct graphite_property { const char* key; // Property key (interned string) graphite_property_value value; // Property value } graphite_property; // // Bundle Management Functions // // Open a GRAPHITE bundle from file graphite_bundle* graphite_open(const char* file_path); // Open with specific load flags and performance monitoring graphite_bundle* graphite_open_ex(const char* file_path, uint32_t load_flags, graphite_performance_stats* stats); // Open from memory buffer (useful for embedded bundles) graphite_bundle* graphite_open_memory(const void* data, size_t size, uint32_t load_flags, graphite_performance_stats* stats); // Close bundle and free all resources void graphite_close(graphite_bundle* bundle); // Get bundle information graphite_bundle_info graphite_get_bundle_info(const graphite_bundle* bundle); // Check if bundle is valid and loaded correctly bool graphite_is_valid(const graphite_bundle* bundle); // // Graph Access Functions // // Get the root graph of the bundle const graphite_graph* graphite_get_root_graph(const graphite_bundle* bundle); // Get number of child nodes in a graph uint32_t graphite_get_node_count(const graphite_graph* graph); // Get number of edges in a graph uint32_t graphite_get_edge_count(const graphite_graph* graph); // Get child node by index const graphite_graph* graphite_get_node(const graphite_graph* graph, uint32_t index); // Get child node by property value (if property exists and matches) const graphite_graph* graphite_find_node_by_property(const graphite_graph* graph, const char* property_key, const char* property_value); // Check if graph has cycles bool graphite_has_cycles(const graphite_graph* graph); // Check if graph is marked as parallel group bool graphite_is_parallel_group(const graphite_graph* graph); // Check if graph is a string pool bool graphite_is_string_pool(const graphite_graph* graph); // // Edge Access Functions // // Get edge by index typedef struct { uint32_t from_node_index; // Source node index uint32_t to_node_index; // Target node index const graphite_graph* edge_data; // Edge semantics graph } graphite_edge; graphite_edge graphite_get_edge(const graphite_graph* graph, uint32_t index); // Find edges connecting specific nodes uint32_t graphite_find_edges(const graphite_graph* graph, uint32_t from_node, uint32_t to_node, graphite_edge* edges, uint32_t max_edges); // // Property Access Functions // // Get number of properties in a graph uint32_t graphite_get_property_count(const graphite_graph* graph); // Get property by key bool graphite_get_property(const graphite_graph* graph, const char* key, graphite_property_value* value); // Get property by index graphite_property graphite_get_property_by_index(const graphite_graph* graph, uint32_t index); // Get typed property values (convenience functions) bool graphite_get_string_property(const graphite_graph* graph, const char* key, const char** value); bool graphite_get_integer_property(const graphite_graph* graph, const char* key, int64_t* value); bool graphite_get_float_property(const graphite_graph* graph, const char* key, double* value); bool graphite_get_boolean_property(const graphite_graph* graph, const char* key, bool* value); bool graphite_get_timestamp_property(const graphite_graph* graph, const char* key, uint64_t* value); // Check if property exists bool graphite_has_property(const graphite_graph* graph, const char* key); // // String Pool Functions // // Get string by ID from bundle's string pool const char* graphite_get_string(const graphite_bundle* bundle, uint32_t string_id); // Find string ID by value (reverse lookup) uint32_t graphite_find_string_id(const graphite_bundle* bundle, const char* string); // Get string pool statistics typedef struct { uint32_t string_count; // Number of unique strings size_t total_bytes; // Total bytes of string data uint32_t average_length; // Average string length double deduplication_ratio; // Space saved by deduplication } graphite_string_pool_stats; graphite_string_pool_stats graphite_get_string_pool_stats(const graphite_bundle* bundle); // // Asset Data Access Functions // // Get raw asset data from a leaf graph const void* graphite_get_asset_data(const graphite_graph* asset_graph, size_t* size); // Get asset MIME type const char* graphite_get_asset_mime_type(const graphite_graph* asset_graph); // Get asset original size (before compression) uint64_t graphite_get_asset_original_size(const graphite_graph* asset_graph); // Get asset checksum const char* graphite_get_asset_checksum(const graphite_graph* asset_graph); // // Iterator Functions // // Create node iterator graphite_node_iterator* graphite_create_node_iterator(const graphite_graph* graph); void graphite_destroy_node_iterator(graphite_node_iterator* iterator); // Iterator operations bool graphite_node_iterator_has_next(graphite_node_iterator* iterator); const graphite_graph* graphite_node_iterator_next(graphite_node_iterator* iterator); void graphite_node_iterator_reset(graphite_node_iterator* iterator); // Create edge iterator graphite_edge_iterator* graphite_create_edge_iterator(const graphite_graph* graph); void graphite_destroy_edge_iterator(graphite_edge_iterator* iterator); // Edge iterator operations bool graphite_edge_iterator_has_next(graphite_edge_iterator* iterator); graphite_edge graphite_edge_iterator_next(graphite_edge_iterator* iterator); void graphite_edge_iterator_reset(graphite_edge_iterator* iterator); // // Utility Functions // // Get last error that occurred graphite_error graphite_get_last_error(void); // Get human-readable error message const char* graphite_error_string(graphite_error error); // Get library version information typedef struct { uint32_t major; uint32_t minor; uint32_t patch; const char* string; const char* build_date; const char* build_config; } graphite_version_info; graphite_version_info graphite_get_version_info(void); // Performance profiling void graphite_enable_profiling(bool enable); bool graphite_is_profiling_enabled(void); // Memory usage information typedef struct { size_t arena_size; // Arena allocation size size_t arena_used; // Arena bytes used size_t peak_memory; // Peak memory usage size_t current_memory; // Current memory usage uint32_t allocation_count; // Number of allocations } graphite_memory_stats; graphite_memory_stats graphite_get_memory_stats(const graphite_bundle* bundle); #ifdef __cplusplus } #endif #endif // GRAPHITE_CORE_H ``` ### Usage Examples #### Basic Bundle Loading ```c #include \"graphite_core.h\" #include <stdio.h> int main() { // Open bundle with default settings graphite_bundle* bundle = graphite_open(\"assets.graphite\"); if (!bundle) { fprintf(stderr, \"Failed to open bundle: %s\ \", graphite_error_string(graphite_get_last_error())); return 1; } // Get bundle information graphite_bundle_info info = graphite_get_bundle_info(bundle); printf(\"Bundle version: %u\ \", info.version); printf(\"File size: %llu bytes\ \", info.file_size); printf(\"Chunks: %u\ \", info.chunk_count); // Access root graph const graphite_graph* root = graphite_get_root_graph(bundle); printf(\"Root graph has %u nodes\ \", graphite_get_node_count(root)); // Iterate through child nodes for (uint32_t i = 0; i < graphite_get_node_count(root); i++) { const graphite_graph* child = graphite_get_node(root, i); // Check if this is an asset node const char* mime_type = graphite_get_asset_mime_type(child); if (mime_type) { printf(\"Asset %u: %s\ \", i, mime_type); size_t data_size; const void* data = graphite_get_asset_data(child, &data_size); printf(\" Size: %zu bytes\ \", data_size); } } graphite_close(bundle); return 0; } ``` #### Advanced Loading with Performance Monitoring ```c #include \"graphite_core.h\" #include <stdio.h> int main() { // Enable performance monitoring graphite_performance_stats stats = {0}; uint32_t flags = GRAPHITE_VERIFY_HASHES | GRAPHITE_DECOMPRESS | GRAPHITE_HW_ACCELERATION; graphite_bundle* bundle = graphite_open_ex(\"assets.graphite\", flags, &stats); if (!bundle) { fprintf(stderr, \"Failed to open bundle\ \"); return 1; } // Print performance statistics printf(\"Loading Performance:\ \"); printf(\" Total time: %.2f ms\ \", stats.total_load_time_ns / 1e6); printf(\" CRC verification: %.2f ms\ \", stats.crc_verification_time_ns / 1e6); printf(\" Decompression: %.2f ms\ \", stats.decompression_time_ns / 1e6); printf(\" Hash verification: %.2f ms\ \", stats.hash_verification_time_ns / 1e6); printf(\" Hydration: %.2f ms\ \", stats.hydration_time_ns / 1e6); printf(\" Bytes processed: %llu\ \", stats.total_bytes_processed); printf(\" Peak memory: %.2f MB\ \", stats.peak_memory_usage / (1024.0 * 1024.0)); printf(\" Compression ratio: %.2f%%\ \", stats.compression_ratio * 100.0); // Use bundle... graphite_close(bundle); return 0; } ``` #### Property Access and Filtering ```c #include \"graphite_core.h\" #include <stdio.h> #include <string.h> void print_graph_properties(const graphite_graph* graph, const char* indent) { uint32_t prop_count = graphite_get_property_count(graph); for (uint32_t i = 0; i < prop_count; i++) { graphite_property prop = graphite_get_property_by_index(graph, i); printf(\"%s%s: \", indent, prop.key); switch (prop.value.type) { case GRAPHITE_PROPERTY_STRING: printf(\"\\\"%s\\\"\", prop.value.string_value); break; case GRAPHITE_PROPERTY_INTEGER: printf(\"%lld\", prop.value.integer_value); break; case GRAPHITE_PROPERTY_FLOAT: printf(\"%.6f\", prop.value.float_value); break; case GRAPHITE_PROPERTY_BOOLEAN: printf(\"%s\", prop.value.boolean_value ? \"true\" : \"false\"); break; case GRAPHITE_PROPERTY_TIMESTAMP: printf(\"timestamp(%llu)\", prop.value.timestamp_value); break; default: printf(\"(complex type)\"); break; } printf(\"\ \"); } } int main() { graphite_bundle* bundle = graphite_open(\"assets.graphite\"); if (!bundle) return 1; const graphite_graph* root = graphite_get_root_graph(bundle); // Find all image assets for (uint32_t i = 0; i < graphite_get_node_count(root); i++) { const graphite_graph* node = graphite_get_node(root, i); const char* mime_type; if (graphite_get_string_property(node, \"mime_type\", &mime_type)) { if (strncmp(mime_type, \"image/\", 6) == 0) { printf(\"Image Asset %u (%s):\ \", i, mime_type); print_graph_properties(node, \" \"); printf(\"\ \"); } } } graphite_close(bundle); return 0; } ``` --- ## Tooling API Specification ### Bundle Creation API (graphite_tooling.h) The tooling API provides comprehensive functionality for creating, analyzing, and manipulating GRAPHITE bundles: ```c #ifndef GRAPHITE_TOOLING_H #define GRAPHITE_TOOLING_H #include \"graphite_core.h\" #include <stdint.h> #include <stdbool.h> #ifdef __cplusplus extern \"C\" { #endif // Forward declarations for tooling types typedef struct graphite_writer graphite_writer; typedef struct graphite_graph_builder graphite_graph_builder; typedef struct graphite_string_pool_builder graphite_string_pool_builder; typedef struct graphite_integrity_builder graphite_integrity_builder; typedef struct graphite_analyzer graphite_analyzer; // // Bundle Writer Configuration // // Compression configuration typedef struct { int level; // zstd compression level (0-22) bool enable_dictionary; // Use dictionary compression size_t dictionary_size; // Maximum dictionary size size_t min_chunk_size; // Minimum size to attempt compression double min_compression_ratio; // Minimum compression ratio to keep const char** file_patterns; // File patterns for dictionary training uint32_t pattern_count; // Number of patterns } graphite_compression_config; // Integrity configuration typedef struct { bool enable_hashing; // Enable BLAKE3 hash tree bool mandatory_verification; // Set mandatory verification flag int tree_fanout; // Hash tree branching factor (2-32) bool include_chunk_hashes; // Include individual chunk hashes } graphite_integrity_config; // Output configuration typedef struct { const char* output_path; // Output file path bool overwrite_existing; // Overwrite if file exists uint32_t file_permissions; // File permissions (Unix style) bool create_backup; // Create .bak file if overwriting } graphite_output_config; // Writer statistics typedef struct { uint64_t total_input_size; // Total input data size uint64_t total_output_size; // Total output file size uint32_t chunks_written; // Number of chunks written uint32_t chunks_compressed; // Number of compressed chunks uint32_t graphs_created; // Number of graph chunks created uint32_t blobs_created; // Number of blob chunks created double overall_compression_ratio; // Overall compression effectiveness uint64_t write_time_ns; // Total write time uint64_t compression_time_ns; // Time spent compressing uint64_t hashing_time_ns; // Time spent computing hashes } graphite_writer_stats; // // Bundle Writer Functions // // Create bundle writer with configuration graphite_writer* graphite_writer_create(const graphite_output_config* output_config); // Configure compression settings void graphite_writer_set_compression(graphite_writer* writer, const graphite_compression_config* config); // Configure integrity settings void graphite_writer_set_integrity(graphite_writer* writer, const graphite_integrity_config* config); // Set metadata for the bundle void graphite_writer_set_metadata(graphite_writer* writer, const char* key, const char* value); // Finalize and write bundle to disk bool graphite_writer_finalize(graphite_writer* writer, graphite_writer_stats* stats); // Destroy writer and free resources void graphite_writer_destroy(graphite_writer* writer); // // Graph Builder Functions // // Create new graph builder graphite_graph_builder* graphite_graph_builder_create(void); // Set graph flags void graphite_graph_builder_set_flags(graphite_graph_builder* builder, uint32_t flags); // Add asset node (leaf graph with data) uint32_t graphite_graph_builder_add_asset_node(graphite_graph_builder* builder, const void* data, size_t size, const char* mime_type); // Add asset node from file uint32_t graphite_graph_builder_add_asset_file(graphite_graph_builder* builder, const char* file_path, const char* mime_type); // Add child graph node uint32_t graphite_graph_builder_add_graph_node(graphite_graph_builder* builder, const graphite_graph* subgraph); // Add edge between nodes void graphite_graph_builder_add_edge(graphite_graph_builder* builder, uint32_t from_node, uint32_t to_node, const graphite_graph* edge_semantics); // Add simple edge with metadata void graphite_graph_builder_add_simple_edge(graphite_graph_builder* builder, uint32_t from_node, uint32_t to_node, const char* edge_type, const char* metadata); // Set property on graph void graphite_graph_builder_set_property_string(graphite_graph_builder* builder, const char* key, const char* value); void graphite_graph_builder_set_property_integer(graphite_graph_builder* builder, const char* key, int64_t value); void graphite_graph_builder_set_property_float(graphite_graph_builder* builder, const char* key, double value); void graphite_graph_builder_set_property_boolean(graphite_graph_builder* builder, const char* key, bool value); void graphite_graph_builder_set_property_timestamp(graphite_graph_builder* builder, const char* key, uint64_t value); // Finalize graph and add to writer uint32_t graphite_writer_add_graph(graphite_writer* writer, graphite_graph_builder* builder); // Destroy graph builder void graphite_graph_builder_destroy(graphite_graph_builder* builder); // // String Pool Builder Functions // // Create string pool builder graphite_string_pool_builder* graphite_string_pool_builder_create(void); // Add string to pool (returns string ID) uint32_t graphite_string_pool_add_string(graphite_string_pool_builder* builder, const char* string); // Add strings from array void graphite_string_pool_add_strings(graphite_string_pool_builder* builder, const char** strings, uint32_t count); // Load strings from file (one per line) bool graphite_string_pool_load_from_file(graphite_string_pool_builder* builder, const char* file_path); // Get string pool statistics typedef struct { uint32_t unique_strings; // Number of unique strings uint32_t total_references; // Total string references size_t total_bytes; // Total string data bytes double deduplication_ratio; // Space saved by deduplication } graphite_string_pool_builder_stats; graphite_string_pool_builder_stats graphite_string_pool_get_stats(graphite_string_pool_builder* builder); // Finalize string pool and add to writer uint32_t graphite_writer_set_string_pool(graphite_writer* writer, graphite_string_pool_builder* builder); // Destroy string pool builder void graphite_string_pool_builder_destroy(graphite_string_pool_builder* builder); // // Compression Dictionary Training // // Dictionary training configuration typedef struct { size_t max_dictionary_size; // Maximum dictionary size uint32_t sample_count; // Number of training samples size_t max_sample_size; // Maximum size per sample double min_compression_gain; // Minimum compression improvement } graphite_dictionary_config; // Train compression dictionary from files bool graphite_train_dictionary(const char** file_paths, uint32_t file_count, const graphite_dictionary_config* config, void** dictionary_data, size_t* dictionary_size); // Train dictionary from memory samples bool graphite_train_dictionary_memory(const void** samples, const size_t* sample_sizes, uint32_t sample_count, const graphite_dictionary_config* config, void** dictionary_data, size_t* dictionary_size); // Add trained dictionary to writer void graphite_writer_set_dictionary(graphite_writer* writer, const void* dictionary_data, size_t dictionary_size); // // Bundle Analysis Functions // // Create bundle analyzer graphite_analyzer* graphite_analyzer_create(const graphite_bundle* bundle); // Analysis result types typedef struct { uint32_t total_graphs; // Total number of graphs uint32_t leaf_graphs; // Number of leaf (asset) graphs uint32_t string_pools; // Number of string pool graphs uint32_t parallel_groups; // Number of parallel group graphs uint32_t max_depth; // Maximum graph nesting depth uint32_t total_nodes; // Total nodes across all graphs uint32_t total_edges; // Total edges across all graphs double avg_branching_factor; // Average branching factor bool has_cycles; // Whether any graph has cycles } graphite_graph_analysis; typedef struct { uint64_t total_size; // Total bundle size uint64_t header_size; // Header and chunk table size uint64_t chunk_data_size; // Total chunk data size uint64_t compressed_size; // Size of compressed chunks uint64_t uncompressed_size; // Size before compression double compression_ratio; // Overall compression ratio uint32_t compression_chunks; // Number of compressed chunks uint32_t compression_failures; // Chunks that didn't compress well } graphite_compression_analysis; typedef struct { bool has_integrity_tree; // Has hash tree uint32_t hash_nodes; // Number of hash nodes uint32_t hash_leaves; // Number of hash leaves uint32_t protected_chunks; // Number of protected chunks double verification_coverage; // Percentage of data protected bool mandatory_verification; // Verification is mandatory } graphite_integrity_analysis; typedef struct { uint32_t unique_strings; // Number of unique strings uint32_t total_references; // Total string references size_t string_data_size; // Total string data size double deduplication_ratio; // Space saved by deduplication uint32_t unused_strings; // Strings with no references double fragmentation_ratio; // String pool fragmentation } graphite_string_analysis; // Perform various analyses graphite_graph_analysis graphite_analyzer_analyze_graphs(graphite_analyzer* analyzer); graphite_compression_analysis graphite_analyzer_analyze_compression(graphite_analyzer* analyzer); graphite_integrity_analysis graphite_analyzer_analyze_integrity(graphite_analyzer* analyzer); graphite_string_analysis graphite_analyzer_analyze_strings(graphite_analyzer* analyzer); // Generate optimization recommendations typedef struct { bool enable_compression; // Recommendation to enable compression int suggested_compression_level; // Suggested compression level bool enable_dictionary; // Recommendation for dictionary training bool improve_string_dedup; // String deduplication improvements bool add_integrity_tree; // Add hash tree recommendation double potential_size_reduction; // Estimated size reduction percentage const char* primary_recommendation; // Primary optimization suggestion const char** detailed_recommendations; // Detailed suggestion list uint32_t recommendation_count; // Number of detailed recommendations } graphite_optimization_recommendations; graphite_optimization_recommendations graphite_analyzer_get_recommendations(graphite_analyzer* analyzer); // Destroy analyzer void graphite_analyzer_destroy(graphite_analyzer* analyzer); // // Bundle Comparison and Diffing // // Bundle comparison result typedef struct { bool structure_identical; // Same graph structure bool content_identical; // Same content hashes uint32_t added_graphs; // Number of added graphs uint32_t removed_graphs; // Number of removed graphs uint32_t modified_graphs; // Number of modified graphs uint32_t added_chunks; // Number of added chunks uint32_t removed_chunks; // Number of removed chunks uint32_t modified_chunks; // Number of modified chunks uint64_t size_difference; // Size difference in bytes } graphite_comparison_result; // Compare two bundles graphite_comparison_result graphite_compare_bundles(const graphite_bundle* bundle1, const graphite_bundle* bundle2); // Generate binary diff between bundles typedef struct { const void* diff_data; // Binary diff data size_t diff_size; // Size of diff data double compression_ratio; // Diff compression ratio bool applicable; // Whether diff can be applied } graphite_binary_diff; graphite_binary_diff graphite_create_binary_diff(const graphite_bundle* old_bundle, const graphite_bundle* new_bundle); // Apply binary diff to create new bundle bool graphite_apply_binary_diff(const graphite_bundle* base_bundle, const graphite_binary_diff* diff, const char* output_path); // Free binary diff resources void graphite_free_binary_diff(graphite_binary_diff* diff); // // Validation and Repair Functions // // Validation levels typedef enum { GRAPHITE_VALIDATE_BASIC, // Basic format validation GRAPHITE_VALIDATE_STRUCTURE, // Graph structure validation GRAPHITE_VALIDATE_INTEGRITY, // Hash verification GRAPHITE_VALIDATE_COMPREHENSIVE // All validation checks } graphite_validation_level; // Validation result typedef struct { bool is_valid; // Overall validation result uint32_t error_count; // Number of errors found uint32_t warning_count; // Number of warnings const char** errors; // Array of error messages const char** warnings; // Array of warning messages bool can_be_repaired; // Whether bundle can be repaired } graphite_validation_result; // Validate bundle graphite_validation_result graphite_validate_bundle(const char* bundle_path, graphite_validation_level level); // Attempt to repair bundle bool graphite_repair_bundle(const char* input_path, const char* output_path, const graphite_validation_result* validation); // Free validation result void graphite_free_validation_result(graphite_validation_result* result); // // Format Conversion Functions // // Supported export formats typedef enum { GRAPHITE_EXPORT_JSON, // JSON representation GRAPHITE_EXPORT_XML, // XML representation GRAPHITE_EXPORT_YAML, // YAML representation GRAPHITE_EXPORT_DOT, // Graphviz DOT format GRAPHITE_EXPORT_MERMAID, // Mermaid diagram format GRAPHITE_EXPORT_CSV // CSV for tabular data } graphite_export_format; // Export configuration typedef struct { graphite_export_format format; // Target format bool include_binary_data; // Include base64-encoded binary data bool pretty_print; // Format for human readability uint32_t max_depth; // Maximum nesting depth const char* filter_expression; // Filter expression for selective export } graphite_export_config; // Export bundle to different format bool graphite_export_bundle(const graphite_bundle* bundle, const char* output_path, const graphite_export_config* config); // Import from supported formats graphite_graph_builder* graphite_import_from_json(const char* json_path); graphite_graph_builder* graphite_import_from_xml(const char* xml_path); graphite_graph_builder* graphite_import_from_yaml(const char* yaml_path); #ifdef __cplusplus } #endif #endif // GRAPHITE_TOOLING_H ``` ### High-Level Builder Interface #### Bundle Creation Example ```c #include \"graphite_tooling.h\" #include <stdio.h> int create_game_assets_bundle() { // Configure output graphite_output_config output = { .output_path = \"game_assets.graphite\", .overwrite_existing = true, .create_backup = true }; // Create writer graphite_writer* writer = graphite_writer_create(&output); if (!writer) { fprintf(stderr, \"Failed to create writer\ \"); return 1; } // Configure compression graphite_compression_config compression = { .level = 5, .enable_dictionary = true, .min_chunk_size = 64 * 1024, .min_compression_ratio = 0.9 }; graphite_writer_set_compression(writer, &compression); // Configure integrity graphite_integrity_config integrity = { .enable_hashing = true, .mandatory_verification = true, .tree_fanout = 8 }; graphite_writer_set_integrity(writer, &integrity); // Create string pool graphite_string_pool_builder* strings = graphite_string_pool_builder_create(); // Create root graph graphite_graph_builder* root = graphite_graph_builder_create(); graphite_graph_builder_set_property_string(root, \"bundle_type\", \"game_assets\"); graphite_graph_builder_set_property_string(root, \"version\", \"1.0.0\"); // Create textures subgraph graphite_graph_builder* textures = graphite_graph_builder_create(); graphite_graph_builder_set_property_string(textures, \"category\", \"textures\"); // Add texture assets uint32_t player_texture = graphite_graph_builder_add_asset_file( textures, \"assets/player.png\", \"image/png\"); uint32_t ui_atlas = graphite_graph_builder_add_asset_file( textures, \"assets/ui_atlas.png\", \"image/png\"); // Create audio subgraph graphite_graph_builder* audio = graphite_graph_builder_create(); graphite_graph_builder_set_property_string(audio, \"category\", \"audio\"); uint32_t music = graphite_graph_builder_add_asset_file( audio, \"assets/background_music.ogg\", \"audio/ogg\"); uint32_t sfx = graphite_graph_builder_add_asset_file( audio, \"assets/jump_sound.wav\", \"audio/wav\"); // Add dependency edges graphite_graph_builder_add_simple_edge(textures, player_texture, ui_atlas, \"spatial_relationship\", \"ui_atlas_contains_player_ui\"); // Add subgraphs to root uint32_t textures_node = graphite_writer_add_graph(writer, textures); uint32_t audio_node = graphite_writer_add_graph(writer, audio); graphite_graph_builder_add_graph_node(root, graphite_get_node(graphite_get_root_graph(bundle), textures_node)); graphite_graph_builder_add_graph_node(root, graphite_get_node(graphite_get_root_graph(bundle), audio_node)); // Set root graph uint32_t root_index = graphite_writer_add_graph(writer, root); // Set string pool graphite_writer_set_string_pool(writer, strings); // Finalize bundle graphite_writer_stats stats; if (graphite_writer_finalize(writer, &stats)) { printf(\"Bundle created successfully!\ \"); printf(\" Input size: %.2f MB\ \", stats.total_input_size / (1024.0 * 1024.0)); printf(\" Output size: %.2f MB\ \", stats.total_output_size / (1024.0 * 1024.0)); printf(\" Compression ratio: %.2f%%\ \", stats.overall_compression_ratio * 100.0); printf(\" Chunks written: %u\ \", stats.chunks_written); } else { fprintf(stderr, \"Failed to finalize bundle\ \"); graphite_writer_destroy(writer); return 1; } // Cleanup graphite_graph_builder_destroy(root); graphite_graph_builder_destroy(textures); graphite_graph_builder_destroy(audio); graphite_string_pool_builder_destroy(strings); graphite_writer_destroy(writer); return 0; } ``` #### Bundle Analysis Example ```c #include \"graphite_tooling.h\" #include <stdio.h> void analyze_bundle(const char* bundle_path) { // Open bundle graphite_bundle* bundle = graphite_open(bundle_path); if (!bundle) { fprintf(stderr, \"Failed to open bundle: %s\ \", bundle_path); return; } // Create analyzer graphite_analyzer* analyzer = graphite_analyzer_create(bundle); // Perform graph analysis graphite_graph_analysis graph_stats = graphite_analyzer_analyze_graphs(analyzer); printf(\"Graph Analysis:\ \"); printf(\" Total graphs: %u\ \", graph_stats.total_graphs); printf(\" Leaf graphs: %u\ \", graph_stats.leaf_graphs); printf(\" String pools: %u\ \", graph_stats.string_pools); printf(\" Parallel groups: %u\ \", graph_stats.parallel_groups); printf(\" Maximum depth: %u\ \", graph_stats.max_depth); printf(\" Total nodes: %u\ \", graph_stats.total_nodes); printf(\" Total edges: %u\ \", graph_stats.total_edges); printf(\" Avg branching factor: %.2f\ \", graph_stats.avg_branching_factor); printf(\" Has cycles: %s\ \", graph_stats.has_cycles ? \"yes\" : \"no\"); // Perform compression analysis graphite_compression_analysis comp_stats = graphite_analyzer_analyze_compression(analyzer); printf(\"\ Compression Analysis:\ \"); printf(\" Total size: %.2f MB\ \", comp_stats.total_size / (1024.0 * 1024.0)); printf(\" Compressed size: %.2f MB\ \", comp_stats.compressed_size / (1024.0 * 1024.0)); printf(\" Compression ratio: %.2f%%\ \", comp_stats.compression_ratio * 100.0); printf(\" Compressed chunks: %u\ \", comp_stats.compression_chunks); printf(\" Compression failures: %u\ \", comp_stats.compression_failures); // Perform integrity analysis graphite_integrity_analysis int_stats = graphite_analyzer_analyze_integrity(analyzer); printf(\"\ Integrity Analysis:\ \"); printf(\" Has integrity tree: %s\ \", int_stats.has_integrity_tree ? \"yes\" : \"no\"); printf(\" Hash nodes: %u\ \", int_stats.hash_nodes); printf(\" Hash leaves: %u\ \", int_stats.hash_leaves); printf(\" Protected chunks: %u\ \", int_stats.protected_chunks); printf(\" Verification coverage: %.2f%%\ \", int_stats.verification_coverage * 100.0); printf(\" Mandatory verification: %s\ \", int_stats.mandatory_verification ? \"yes\" : \"no\"); // Get optimization recommendations graphite_optimization_recommendations recommendations = graphite_analyzer_get_recommendations(analyzer); printf(\"\ Optimization Recommendations:\ \"); printf(\" Primary: %s\ \", recommendations.primary_recommendation); printf(\" Potential size reduction: %.2f%%\ \", recommendations.potential_size_reduction * 100.0); for (uint32_t i = 0; i < recommendations.recommendation_count; i++) { printf(\" - %s\ \", recommendations.detailed_recommendations[i]); } // Cleanup graphite_analyzer_destroy(analyzer); graphite_close(bundle); } ``` --- ## Wrapper APIs ### Unity Integration API ```c #ifndef GRAPHITE_UNITY_H #define GRAPHITE_UNITY_H #include \"graphite_core.h\" #ifdef __cplusplus extern \"C\" { #endif // Unity-specific asset types typedef enum { UNITY_ASSET_TEXTURE2D, UNITY_ASSET_SPRITE, UNITY_ASSET_MESH, UNITY_ASSET_MATERIAL, UNITY_ASSET_SHADER, UNITY_ASSET_AUDIO_CLIP, UNITY_ASSET_SCRIPT, UNITY_ASSET_PREFAB, UNITY_ASSET_SCENE, UNITY_ASSET_ANIMATION, UNITY_ASSET_FONT } unity_asset_type; // Unity bundle handle typedef struct unity_graphite_bundle unity_graphite_bundle; // Unity-specific loading flags typedef enum { UNITY_LOAD_IMMEDIATE = 0, // Load all assets immediately UNITY_LOAD_STREAMING = 1 << 0, // Enable streaming for large assets UNITY_LOAD_ASYNC = 1 << 1, // Asynchronous loading UNITY_ENABLE_PROFILING = 1 << 2, // Unity Profiler integration UNITY_ENABLE` }``

``{ `content`: `# GRAPHITE Asset Graph Format Specification v3.0 - CONTINUED [Previous content continues...] ```c // Unity-specific loading flags typedef enum { UNITY_LOAD_IMMEDIATE = 0, // Load all assets immediately UNITY_LOAD_STREAMING = 1 << 0, // Enable streaming for large assets UNITY_LOAD_ASYNC = 1 << 1, // Asynchronous loading UNITY_ENABLE_PROFILING = 1 << 2, // Unity Profiler integration UNITY_ENABLE_HOT_RELOAD = 1 << 3, // Hot reload for development UNITY_OPTIMIZE_BATCHING = 1 << 4, // Optimize for Unity batching UNITY_ENABLE_ADDRESSABLES = 1 << 5 // Addressable asset integration } unity_load_flags; // Unity asset request typedef struct { const char* asset_path; // Asset path or GUID unity_asset_type expected_type; // Expected Unity asset type int priority; // Loading priority void* user_data; // User-defined data void (*callback)(struct unity_asset_request*, void* asset, bool success); } unity_asset_request; // Unity streaming configuration typedef struct { size_t memory_budget; // Memory budget for streaming uint32_t max_concurrent_loads; // Maximum concurrent load operations float distance_threshold; // Distance-based loading threshold bool enable_lod_streaming; // Level-of-detail based streaming } unity_streaming_config; // // Unity Bundle Management // // Open GRAPHITE bundle for Unity unity_graphite_bundle* unity_graphite_open(const char* bundle_path, uint32_t unity_flags, const unity_streaming_config* streaming); // Close Unity bundle void unity_graphite_close(unity_graphite_bundle* bundle); // Check if bundle is valid bool unity_graphite_is_valid(const unity_graphite_bundle* bundle); // // Unity Asset Loading // // Load asset synchronously (returns Unity Object*) void* unity_graphite_load_asset(unity_graphite_bundle* bundle, const char* asset_path, unity_asset_type type); // Load asset asynchronously uint32_t unity_graphite_load_asset_async(unity_graphite_bundle* bundle, const unity_asset_request* request); // Cancel async load operation bool unity_graphite_cancel_async_load(unity_graphite_bundle* bundle, uint32_t operation_id); // Check async load status typedef enum { UNITY_LOAD_PENDING, UNITY_LOAD_IN_PROGRESS, UNITY_LOAD_COMPLETED, UNITY_LOAD_FAILED, UNITY_LOAD_CANCELLED } unity_load_status; unity_load_status unity_graphite_get_load_status(unity_graphite_bundle* bundle, uint32_t operation_id); // // Unity-Specific Asset Types // // Create Unity Texture2D from GRAPHITE asset void* unity_graphite_create_texture2d(const graphite_graph* asset_graph, int width, int height, int texture_format, bool generate_mipmaps); // Create Unity Sprite from GRAPHITE asset void* unity_graphite_create_sprite(const graphite_graph* asset_graph, void* texture, float pixels_per_unit); // Create Unity Mesh from GRAPHITE asset void* unity_graphite_create_mesh(const graphite_graph* asset_graph); // Create Unity AudioClip from GRAPHITE asset void* unity_graphite_create_audio_clip(const graphite_graph* asset_graph, const char* clip_name); // // Unity Addressable Integration // // Register GRAPHITE bundle with Addressables bool unity_graphite_register_addressables(unity_graphite_bundle* bundle, const char* addressable_group); // Load asset by addressable key void* unity_graphite_load_by_address(unity_graphite_bundle* bundle, const char* address); // Load asset by addressable key (async) uint32_t unity_graphite_load_by_address_async(unity_graphite_bundle* bundle, const char* address, void (*callback)(void* asset, bool success)); // // Unity Profiler Integration // // Begin profiler sample void unity_graphite_profiler_begin_sample(const char* sample_name); // End profiler sample void unity_graphite_profiler_end_sample(void); // Report memory usage to Unity Profiler void unity_graphite_profiler_report_memory(const char* category, size_t bytes_used); // // Unity Hot Reload Support // // Enable hot reload monitoring bool unity_graphite_enable_hot_reload(unity_graphite_bundle* bundle, const char* source_directory); // Check for bundle changes bool unity_graphite_has_changes(unity_graphite_bundle* bundle); // Reload bundle with changes bool unity_graphite_reload_bundle(unity_graphite_bundle* bundle); // Hot reload callback typedef void (*unity_hot_reload_callback)(unity_graphite_bundle* bundle, const char* changed_asset_path, void* user_data); void unity_graphite_set_hot_reload_callback(unity_graphite_bundle* bundle, unity_hot_reload_callback callback, void* user_data); // // Unity Streaming Support // // Update streaming based on camera position void unity_graphite_update_streaming(unity_graphite_bundle* bundle, float camera_x, float camera_y, float camera_z); // Set streaming memory budget void unity_graphite_set_memory_budget(unity_graphite_bundle* bundle, size_t memory_bytes); // Get streaming statistics typedef struct { uint32_t assets_loaded; // Currently loaded assets uint32_t assets_pending; // Assets queued for loading size_t memory_used; // Current memory usage size_t memory_budget; // Configured memory budget float hit_ratio; // Cache hit ratio } unity_streaming_stats; unity_streaming_stats unity_graphite_get_streaming_stats(unity_graphite_bundle* bundle); #ifdef __cplusplus } #endif #endif // GRAPHITE_UNITY_H ``` ### Unreal Engine Integration API ```c #ifndef GRAPHITE_UNREAL_H #define GRAPHITE_UNREAL_H #include \"graphite_core.h\" #ifdef __cplusplus extern \"C\" { #endif // Unreal-specific asset types typedef enum { UNREAL_ASSET_TEXTURE, UNREAL_ASSET_STATIC_MESH, UNREAL_ASSET_SKELETAL_MESH, UNREAL_ASSET_MATERIAL, UNREAL_ASSET_SOUND_WAVE, UNREAL_ASSET_BLUEPRINT, UNREAL_ASSET_LEVEL, UNREAL_ASSET_ANIMATION, UNREAL_ASSET_PARTICLE_SYSTEM, UNREAL_ASSET_CURVE_TABLE } unreal_asset_type; // Unreal bundle handle typedef struct unreal_graphite_bundle unreal_graphite_bundle; // Unreal-specific loading flags typedef enum { UNREAL_LOAD_IMMEDIATE = 0, // Load all assets immediately UNREAL_ENABLE_STREAMING = 1 << 0, // Enable World Partition streaming UNREAL_ENABLE_ASYNC = 1 << 1, // Asynchronous loading UNREAL_ENABLE_INSIGHTS = 1 << 2, // Unreal Insights integration UNREAL_ENABLE_HOT_RELOAD = 1 << 3, // Hot reload for development UNREAL_OPTIMIZE_RENDERING = 1 << 4, // Optimize for Unreal rendering UNREAL_ENABLE_NANITE = 1 << 5, // Nanite virtualized geometry UNREAL_ENABLE_LUMEN = 1 << 6 // Lumen dynamic global illumination } unreal_load_flags; // Unreal streaming configuration typedef struct { float streaming_distance; // Streaming distance threshold uint32_t max_concurrent_loads; // Maximum concurrent operations size_t memory_pool_size; // Memory pool for streaming bool enable_world_partition; // World Partition integration bool enable_hlod; // Hierarchical LOD support } unreal_streaming_config; // Unreal asset factory configuration typedef struct { bool auto_generate_lods; // Automatically generate LODs bool enable_collision; // Generate collision meshes bool optimize_for_mobile; // Mobile optimization int texture_compression_quality; // Texture compression quality bool generate_lightmap_uvs; // Generate lightmap UVs } unreal_factory_config; // // Unreal Bundle Management // // Open GRAPHITE bundle for Unreal unreal_graphite_bundle* unreal_graphite_open(const char* bundle_path, uint32_t unreal_flags, const unreal_streaming_config* streaming); // Close Unreal bundle void unreal_graphite_close(unreal_graphite_bundle* bundle); // Check if bundle is valid bool unreal_graphite_is_valid(const unreal_graphite_bundle* bundle); // // Unreal Asset Loading // // Load asset as UObject* void* unreal_graphite_load_asset(unreal_graphite_bundle* bundle, const char* asset_path, unreal_asset_type type, const unreal_factory_config* factory_config); // Load asset asynchronously typedef struct { uint32_t operation_id; // Unique operation identifier const char* asset_path; // Asset path being loaded unreal_asset_type type; // Asset type void* user_data; // User-defined data void (*callback)(struct unreal_async_load*, void* asset, bool success); } unreal_async_load; uint32_t unreal_graphite_load_asset_async(unreal_graphite_bundle* bundle, const unreal_async_load* load_request); // Cancel async operation bool unreal_graphite_cancel_async_load(unreal_graphite_bundle* bundle, uint32_t operation_id); // // Unreal-Specific Asset Factories // // Create UTexture2D from GRAPHITE asset void* unreal_graphite_create_texture2d(const graphite_graph* asset_graph, const unreal_factory_config* config); // Create UStaticMesh from GRAPHITE asset void* unreal_graphite_create_static_mesh(const graphite_graph* asset_graph, const unreal_factory_config* config); // Create USkeletalMesh from GRAPHITE asset void* unreal_graphite_create_skeletal_mesh(const graphite_graph* asset_graph, const unreal_factory_config* config); // Create UMaterial from GRAPHITE asset void* unreal_graphite_create_material(const graphite_graph* asset_graph, const unreal_factory_config* config); // Create USoundWave from GRAPHITE asset void* unreal_graphite_create_sound_wave(const graphite_graph* asset_graph, const unreal_factory_config* config); // // Unreal World Partition Integration // // Register bundle with World Partition bool unreal_graphite_register_world_partition(unreal_graphite_bundle* bundle, void* world_partition); // Load world partition cell bool unreal_graphite_load_world_cell(unreal_graphite_bundle* bundle, int cell_x, int cell_y, float load_distance); // Unload world partition cell bool unreal_graphite_unload_world_cell(unreal_graphite_bundle* bundle, int cell_x, int cell_y); // // Unreal Insights Integration // // Begin Insights event void unreal_graphite_insights_begin_event(const char* event_name, const char* category); // End Insights event void unreal_graphite_insights_end_event(void); // Report memory usage to Insights void unreal_graphite_insights_report_memory(const char* category, size_t bytes_allocated, size_t bytes_used); // // Unreal Blueprint Integration // // Expose GRAPHITE bundle to Blueprint bool unreal_graphite_expose_to_blueprint(unreal_graphite_bundle* bundle, const char* blueprint_class_name); // Blueprint-callable asset loading void* unreal_graphite_bp_load_asset(unreal_graphite_bundle* bundle, const char* asset_path, const char* asset_class); // Blueprint-callable async loading void unreal_graphite_bp_load_asset_async(unreal_graphite_bundle* bundle, const char* asset_path, const char* asset_class, void* delegate); // // Unreal Nanite Integration // // Check if asset supports Nanite bool unreal_graphite_supports_nanite(const graphite_graph* asset_graph); // Create Nanite-enabled static mesh void* unreal_graphite_create_nanite_mesh(const graphite_graph* asset_graph, const unreal_factory_config* config); // Configure Nanite settings typedef struct { float position_precision; // Position precision float normal_precision; // Normal precision float tangent_precision; // Tangent precision uint32_t target_triangles_per_cluster; // Target triangles per cluster bool enable_proxy_mesh; // Enable proxy mesh generation } unreal_nanite_config; void unreal_graphite_set_nanite_config(const unreal_nanite_config* config); // // Unreal Lumen Integration // // Check if asset supports Lumen bool unreal_graphite_supports_lumen(const graphite_graph* asset_graph); // Configure Lumen settings for asset typedef struct { bool enable_surface_cache; // Enable surface cache bool enable_far_field; // Enable far field representation float lumen_card_resolution; // Lumen card resolution bool enable_two_sided_geometry; // Two-sided geometry support } unreal_lumen_config; void unreal_graphite_configure_lumen(const graphite_graph* asset_graph, const unreal_lumen_config* config); // // Unreal Hot Reload Support // // Enable hot reload for development bool unreal_graphite_enable_hot_reload(unreal_graphite_bundle* bundle, const char* source_directory); // Hot reload callback for Unreal typedef void (*unreal_hot_reload_callback)(unreal_graphite_bundle* bundle, const char* changed_asset_path, void* unreal_object, void* user_data); void unreal_graphite_set_hot_reload_callback(unreal_graphite_bundle* bundle, unreal_hot_reload_callback callback, void* user_data); // Reload specific asset bool unreal_graphite_reload_asset(unreal_graphite_bundle* bundle, const char* asset_path); // // Unreal Performance Integration // // Get Unreal-specific performance stats typedef struct { uint32_t assets_loaded; // Currently loaded assets uint32_t blueprint_instances; // Blueprint instances using assets size_t gpu_memory_used; // GPU memory usage size_t streaming_pool_used; // Streaming pool usage float average_load_time_ms; // Average asset load time uint32_t nanite_meshes; // Number of Nanite meshes uint32_t lumen_surfaces; // Number of Lumen surfaces } unreal_performance_stats; unreal_performance_stats unreal_graphite_get_performance_stats(unreal_graphite_bundle* bundle); #ifdef __cplusplus } #endif #endif // GRAPHITE_UNREAL_H ``` --- ## Command Line Interface ### Core CLI Tool: `graphite` The GRAPHITE command-line interface provides comprehensive bundle management, analysis, and debugging capabilities. #### Installation and Setup ```bash # Install GRAPHITE CLI curl -fsSL https://graphite.dev/install.sh | sh # Verify installation graphite --version # Output: GRAPHITE CLI v3.0.0 (build 2025-06-28) # Setup shell completion graphite completion bash > /etc/bash_completion.d/graphite graphite completion zsh > ~/.zsh/completions/_graphite ``` #### Basic Usage Patterns ```bash # Show help graphite --help graphite <command> --help # Global options graphite --verbose # Enable verbose output graphite --quiet # Suppress non-error output graphite --config FILE # Use specific config file graphite --profile # Enable performance profiling ``` ### File Operations #### Bundle Information ```bash # Basic information graphite info bundle.graphite # Output: # GRAPHITE Bundle Information # ═══════════════════════════════ # File: bundle.graphite # Version: 3.0 # Size: 1.2 GB (1,287,651,200 bytes) # Created: 2025-06-28 14:30:22 UTC # # Structure: # Chunks: 1,024 # Graphs: 256 # Blobs: 768 # Hash nodes: 64 # # Compression: # Compressed chunks: 512 (50.0%) # Original size: 2.1 GB # Compression ratio: 61.3% # Dictionary: Yes (32 KB) # # Integrity: # Hash tree: Yes (BLAKE3) # Protected chunks: 1,024 (100%) # Mandatory verification: Yes # Detailed information with statistics graphite info --detailed bundle.graphite # Additional output: # Performance Characteristics: # Estimated load time: 182ms (8-core system) # Memory requirements: 156 MB arena # Cache efficiency: 87% # # Content Analysis: # Image assets: 234 files (780 MB) # Audio assets: 45 files (120 MB) # Mesh assets: 12 files (89 MB) # Script assets: 156 files (3.2 MB) # Other assets: 67 files (12 MB) # JSON output for scripting graphite info --format json bundle.graphite > bundle_info.json # Specific sections only graphite info --section compression bundle.graphite graphite info --section integrity bundle.graphite graphite info --section performance bundle.graphite ``` #### Bundle Listing and Exploration ```bash # List root level contents graphite ls bundle.graphite # Output: # / # ├── textures/ [Graph, 234 assets, 780 MB] # ├── audio/ [Graph, 45 assets, 120 MB] # ├── models/ [Graph, 12 assets, 89 MB] # ├── scripts/ [Graph, 156 assets, 3.2 MB] # └── metadata/ [Graph, 67 assets, 12 MB] # List specific path graphite ls bundle.graphite /textures # Output: # /textures/ # ├── characters/ [Graph, 89 assets] # │ ├── player.png [Asset, image/png, 2.1 MB] # │ ├── enemy.png [Asset, image/png, 1.8 MB] # │ └── npc/ [Graph, 86 assets] # ├── environment/ [Graph, 145 assets] # └── ui/ [Graph, 12 assets] # Tree view with full hierarchy graphite ls --tree bundle.graphite graphite ls --tree --depth 3 bundle.graphite # Show file sizes and details graphite ls --long bundle.graphite /textures # Output: # -rw-r--r-- 2,187,264 2025-06-28 14:30 player.png [image/png] # -rw-r--r-- 1,834,521 2025-06-28 14:30 enemy.png [image/png] # drwxr-xr-x - 2025-06-28 14:30 npc/ [Graph] # Filter by type graphite ls --type asset bundle.graphite graphite ls --type graph bundle.graphite graphite ls --mime-type \"image/*\" bundle.graphite # Search by pattern graphite ls --pattern \"*.png\" bundle.graphite graphite ls --pattern \"*player*\" bundle.graphite ``` #### Asset Extraction ```bash # Extract single asset graphite cat bundle.graphite /textures/player.png > player.png # Extract multiple assets graphite extract bundle.graphite /textures --output textures/ graphite extract bundle.graphite /audio/*.ogg --output audio/ # Extract entire bundle graphite extract bundle.graphite --output extracted/ # Extract with filtering graphite extract bundle.graphite --mime-type \"image/png\" --output images/ graphite extract bundle.graphite --larger-than 1MB --output large_files/ # Extract metadata only graphite extract --metadata-only bundle.graphite --output metadata.json # Streaming extraction for large files graphite extract --streaming bundle.graphite /large_video.mp4 --output video.mp4 ``` ### Bundle Creation #### Basic Bundle Creation ```bash # Create bundle from directory graphite pack assets/ output.graphite # With compression graphite pack --compress assets/ output.graphite graphite pack --compress --level 9 assets/ output.graphite # With integrity protection graphite pack --integrity assets/ output.graphite graphite pack --integrity --mandatory assets/ output.graphite # Combined options graphite pack --compress --level 5 --integrity --mandatory \\ assets/ game_assets.graphite # Create with metadata graphite pack --compress --integrity \\ --set \"title=Game Assets\" \\ --set \"version=1.0.0\" \\ --set \"build_date=$(date -u +%Y-%m-%dT%H:%M:%SZ)\" \\ assets/ game_assets.graphite ``` #### Advanced Bundle Creation ```bash # Custom chunk size graphite pack --chunk-size 1MB assets/ output.graphite # Dictionary compression training graphite pack --compress --train-dictionary \\ --dictionary-samples 1000 \\ --dictionary-size 64KB \\ assets/ output.graphite # Exclude patterns graphite pack --exclude \"*.tmp\" --exclude \"*.bak\" \\ assets/ output.graphite # Include only specific types graphite pack --include \"*.png\" --include \"*.jpg\" \\ assets/ textures.graphite # Parallel processing graphite pack --parallel 8 --compress --integrity \\ assets/ output.graphite # Custom graph structure graphite pack --graph-config graph_layout.yaml \\ assets/ output.graphite # Example graph_layout.yaml: # root: # type: collection # children: # - name: textures # pattern: \"*.{png,jpg,jpeg}\" # compress: true # - name: audio # pattern: \"*.{wav,ogg,mp3}\" # compress: false # - name: scripts # pattern: \"*.{js,lua,py}\" # compress: true # dictionary: true ``` ### Verification and Analysis #### Integrity Verification ```bash # Basic verification graphite verify bundle.graphite # Output: # ✓ File header valid # ✓ Chunk table valid # ✓ CRC32 verification passed (1,024/1,024 chunks) # ✓ BLAKE3 hash tree verification passed # ✓ Bundle integrity verified # Quick verification (CRC only) graphite verify --quick bundle.graphite # Detailed verification with chunk-by-chunk results graphite verify --detailed bundle.graphite # Output includes per-chunk verification status # Verify specific chunks graphite verify --chunk 0,1,2,3 bundle.graphite # Parallel verification graphite verify --parallel bundle.graphite # Fix minor issues automatically graphite verify --repair bundle.graphite output_fixed.graphite ``` #### Performance Analysis ```bash # Basic performance test graphite bench bundle.graphite # Output: # GRAPHITE Performance Benchmark # ═══════════════════════════════ # Bundle: bundle.graphite (1.2 GB) # System: 8-core Intel i7-10700K, 32GB RAM, NVMe SSD # # Load Performance: # Total load time: 182.4 ms # CRC verification: 45.2 ms (24.8%) # Decompression: 89.1 ms (48.9%) # Hash verification: 31.7 ms (17.4%) # Hydration: 16.4 ms (9.0%) # # Memory Usage: # Arena size: 156.2 MB # Peak memory: 178.9 MB # Efficiency: 87.3% # # Task Latency (1000 samples): # P50: 0.8 ms # P95: 2.1 ms # P99: 2.8 ms # P99.9: 4.2 ms # Detailed benchmarking graphite bench --detailed --samples 5000 bundle.graphite # Compare with different configurations graphite bench --no-compression bundle.graphite graphite bench --no-integrity bundle.graphite graphite bench --no-parallel bundle.graphite # Platform-specific optimizations graphite bench --enable-simd bundle.graphite graphite bench --enable-numa bundle.graphite graphite bench --huge-pages bundle.graphite # Export results graphite bench --format json bundle.graphite > benchmark.json graphite bench --format csv bundle.graphite > benchmark.csv ``` #### Bundle Analysis ```bash # Comprehensive analysis graphite analyze bundle.graphite # Output: # GRAPHITE Bundle Analysis # ═══════════════════════ # # Graph Structure: # Total graphs: 67 # Leaf graphs: 512 (assets) # Max depth: 4 levels # Avg branching: 7.6 # Cycles detected: No # # Compression Analysis: # Compressed chunks: 512/1024 (50.0%) # Overall ratio: 61.3% # Best ratio: 89.2% (scripts/*.js) # Worst ratio: 12.1% (textures/*.png) # Dictionary effectiveness: 34.2% # # Storage Efficiency: # Fragmentation: 2.3% # Unused strings: 12 # Optimization potential: 15.7% # # Recommendations: # 1. Enable PNG texture compression (save 89 MB) # 2. Retrain dictionary with larger sample (save 12 MB) # 3. Remove unused string references (save 2.1 MB) # 4. Consolidate small chunks (improve 8% load time) # Specific analysis types graphite analyze --compression bundle.graphite graphite analyze --structure bundle.graphite graphite analyze --strings bundle.graphite graphite analyze --performance bundle.graphite # Generate optimization report graphite analyze --recommendations bundle.graphite > optimization_report.md # Visual analysis (requires graphviz) graphite analyze --visualize bundle.graphite output.svg graphite analyze --visualize --format png bundle.graphite graph.png ``` ### Bundle Comparison and Diffing ```bash # Compare two bundles graphite diff old.graphite new.graphite # Output: # Bundle Comparison Report # ═══════════════════════ # # Structure Changes: # Added graphs: 3 # Removed graphs: 1 # Modified graphs: 12 # # Content Changes: # Added assets: 45 # Removed assets: 7 # Modified assets: 89 # # Size Changes: # Old size: 1.2 GB # New size: 1.4 GB # Difference: +167 MB (+13.9%) # # Notable Changes: # + textures/new_character.png (12.3 MB) # + audio/background_music_v2.ogg (34.1 MB) # ~ textures/player.png (2.1 MB → 2.8 MB) # - models/old_weapon.obj (5.7 MB) # Detailed diff with file-by-file comparison graphite diff --detailed old.graphite new.graphite # Create binary diff patch graphite diff --create-patch old.graphite new.graphite changes.patch # Output: Binary patch created: changes.patch (23.4 MB, 14.1% of new bundle) # Apply binary patch graphite patch old.graphite changes.patch updated.graphite # Verify patch application graphite verify-patch old.graphite new.graphite changes.patch # Export diff report graphite diff --format json old.graphite new.graphite > diff_report.json graphite diff --format html old.graphite new.graphite > diff_report.html ``` ### Dictionary Management ```bash # Train compression dictionary graphite train-dict --samples 1000 --size 64KB \\ assets/*.json dict.zstd # Output: # Dictionary Training Report # ═════════════════════════ # Samples processed: 1,000 files # Total sample size: 45.2 MB # Dictionary size: 64.0 KB # Training time: 3.2 seconds # # Effectiveness Test: # Sample compression (no dict): 71.2% # Sample compression (with dict): 84.6% # Improvement: +13.4% # Train from specific file patterns graphite train-dict --pattern \"*.json\" --pattern \"*.xml\" \\ --size 128KB assets/ config_dict.zstd # Test dictionary effectiveness graphite test-dict dict.zstd assets/*.json # Output per file showing compression improvement # Show dictionary statistics graphite dict-info dict.zstd # Output: # Dictionary Information # ════════════════════ # Size: 65,536 bytes # Training samples: 1,000 # Creation date: 2025-06-28 14:30:22 UTC # Entropy: 7.23 bits/byte # Common patterns: 1,247 # # Top patterns: # 1. \"\\\"type\\\":\" (234 occurrences) # 2. \"\\\"name\\\":\" (198 occurrences) # 3. \"\\\"value\\\":\" (156 occurrences) # Merge multiple dictionaries graphite merge-dict dict1.zstd dict2.zstd merged.zstd ``` ### Development and Debugging Tools #### Bundle Validation ```bash # Comprehensive validation graphite validate bundle.graphite # Output: # GRAPHITE Bundle Validation # ═══════════════════════ # # ✓ Format validation passed # ✓ Structure validation passed # ✓ Integrity validation passed # ✓ Performance validation passed # # Warnings: # ⚠ String pool has 12 unused entries (minor) # ⚠ 3 small chunks could be merged (optimization) # # Bundle is valid and production-ready. # Validation levels graphite validate --level basic bundle.graphite graphite validate --level structure bundle.graphite graphite validate --level integrity bundle.graphite graphite validate --level comprehensive bundle.graphite # Fix validation issues graphite validate --fix bundle.graphite fixed.graphite # Validation with custom rules graphite validate --rules validation_rules.yaml bundle.graphite # Example validation_rules.yaml: # rules: # max_chunk_size: 50MB # require_integrity: true # min_compression_ratio: 0.5 # max_graph_depth: 10 # require_metadata: # - title # - version # - created_date ``` #### Hot Reload Development ```bash # Start hot reload server graphite serve --watch assets/ --output bundle.graphite --port 8080 # Output: # GRAPHITE Hot Reload Server # ═══════════════════════ # Watching: assets/ # Output: bundle.graphite # Server: http://localhost:8080 # # Ready for connections. Press Ctrl+C to stop. # Monitor file changes graphite watch assets/ --command \"graphite pack assets/ bundle.graphite\" # Development mode with automatic rebuild graphite dev-mode --input assets/ --output bundle.graphite \\ --compress --integrity # Hot reload with Unity integration graphite serve --unity-integration --watch assets/ \\ --output bundle.graphite --port 8080 # Hot reload with Unreal integration graphite serve --unreal-integration --watch assets/ \\ --output bundle.graphite --port 8080 ``` #### Debug and Inspection Tools ```bash # Raw hex dump graphite dump --hex bundle.graphite graphite dump --hex --offset 1024 --length 256 bundle.graphite # Chunk table inspection graphite dump --chunk-table bundle.graphite # Output: # Chunk Table (1,024 entries) # ═══════════════════════════ # [0000] offset=0x00000080 size=64 kind=GRAPH flags=0x00 crc32=0x12345678 # [0001] offset=0x000000C0 size=2187264 kind=BLOB flags=0x01 crc32=0x87654321 # [0002] offset=0x00215840 size=1834521 kind=BLOB flags=0x01 crc32=0xABCDEF12 # ... # Graph structure inspection graphite dump --graph bundle.graphite /textures # Output: # Graph: /textures # ═════════════════ # Nodes: 234 # Edges: 456 # Properties: 12 # Flags: PARALLEL_GROUP # # Properties: # category = \"textures\" # total_size = 780MB # compression_ratio = 0.623 # # Nodes: # [000] /textures/characters -> chunk 45 # [001] /textures/environment -> chunk 67 # [002] /textures/ui -> chunk 89 # String pool inspection graphite dump --strings bundle.graphite # Output: # String Pool (1,247 strings) # ══════════════════════════ # [0000] \"mime_type\" # [0001] \"image/png\" # [0002] \"size\" # [0003] \"created\" # ... # Hash tree visualization graphite dump --hash-tree bundle.graphite # Output ASCII art of hash tree structure # Performance trace graphite trace bundle.graphite # Output: # GRAPHITE Load Trace # ═══════════════════ # [ 0.0ms] Begin load: bundle.graphite # [ 2.1ms] Header verified # [ 3.8ms] Chunk table loaded # [ 12.4ms] Arena allocated (156 MB) # [ 45.2ms] CRC verification complete # [134.3ms] Decompression complete # [166.0ms] Hash verification complete # [182.4ms] Hydration complete # [182.4ms] Load complete ``` ### Export and Conversion Tools #### Format Conversion ```bash # Export to JSON graphite export --format json bundle.graphite output.json # Export to XML graphite export --format xml bundle.graphite output.xml # Export to YAML graphite export --format yaml bundle.graphite output.yaml # Export graph visualization graphite export --format dot bundle.graphite graph.dot dot -Tpng graph.dot -o graph.png # Export Mermaid diagram graphite export --format mermaid bundle.graphite graph.mmd # CSV export for analysis graphite export --format csv bundle.graphite assets.csv # Include binary data (Base64) graphite export --format json --include-binary bundle.graphite full_export.json # Selective export graphite export --format json --path /textures bundle.graphite textures.json graphite export --format json --type asset bundle.graphite assets.json ``` #### Import from Other Formats ```bash # Import from JSON graphite import --format json assets.json output.graphite # Import Unity AssetBundles graphite import --format unity-assetbundle game.unity3d output.graphite # Import from directory with metadata graphite import --format directory-with-manifest assets/ output.graphite # Import with custom mapping graphite import --format json --mapping import_mapping.yaml \\ assets.json output.graphite ``` ### CI/CD Integration Tools #### Automated Build Pipeline ```bash # Validate build inputs graphite validate-inputs assets/ # Build with CI optimizations graphite pack --ci-mode --parallel --compress --integrity \\ --set \"build_id=$CI_BUILD_ID\" \\ --set \"commit_sha=$CI_COMMIT_SHA\" \\ --set \"build_date=$(date -u +%Y-%m-%dT%H:%M:%SZ)\" \\ assets/ dist/game_assets.graphite # Verify build output graphite verify --comprehensive dist/game_assets.graphite # Generate build report graphite analyze --format json dist/game_assets.graphite > build_report.json # Upload to artifact storage (example with S3) graphite upload --s3-bucket builds --s3-key \"v${VERSION}/game_assets.graphite\" \\ dist/game_assets.graphite ``` #### Quality Gates ```bash # Size regression check graphite size-check --baseline baseline.graphite \\ --current current.graphite \\ --max-increase 5% # Performance regression check graphite perf-check --baseline baseline.graphite \\ --current current.graphite \\ --max-load-time-increase 10% # Security validation graphite security-scan bundle.graphite # Compliance check graphite compliance-check --standard ISO27001 bundle.graphite ``` --- ## Integration Patterns ### Game Engine Integration #### Asset Streaming Architecture Modern games require sophisticated asset streaming to handle large worlds and limited memory. GRAPHITE provides native support for distance-based and priority-based streaming: ```mermaid graph TB subgraph \"Game World\" A[Player Position] B[Camera Frustum] C[Level Boundaries] end subgraph \"Streaming Controller\" D[Distance Calculator] E[Priority Queue] F[Memory Manager] G[Load Scheduler] end subgraph \"GRAPHITE Loader\" H[Chunk Cache] I[Decompression Pool] J[Background Loader] end A --> D B --> D C --> D D --> E E --> F F --> G G --> H H --> I I --> J ``` ```c // Advanced streaming implementation typedef struct { float player_x, player_y, player_z; float view_distance; float camera_fov; float camera_near, camera_far; uint32_t memory_budget_mb; uint32_t priority_levels; } streaming_config; typedef struct { const graphite_graph* asset_graph; float distance_to_player; float importance_score; uint32_t memory_cost; uint64_t last_access_time; enum { UNLOADED, LOADING, LOADED, UNLOADING } state; } streaming_asset; void update_asset_streaming(graphite_bundle* bundle, const streaming_config* config) { // 1. Calculate distances and priorities streaming_asset* assets = calculate_streaming_priorities(bundle, config); // 2. Apply memory pressure management if (get_current_memory_usage() > config->memory_budget_mb * 1024 * 1024) { unload_low_priority_assets(assets); } // 3. Load high-priority assets for (streaming_asset* asset = assets; asset; asset = asset->next) { if (asset->distance_to_player < config->view_distance && asset->state == UNLOADED) { queue_asset_load(asset); } } // 4. Preload based on movement prediction vector3 predicted_position = predict_player_movement(config); preload_assets_near_position(bundle, predicted_position); } // Predictive loading based on player behavior vector3 predict_player_movement(const streaming_config* config) { // Analyze player velocity and acceleration static vector3 position_history[10]; static size_t history_index = 0; // Simple linear prediction (can be enhanced with ML) vector3 velocity = calculate_velocity(position_history); vector3 predicted = { config->player_x + velocity.x * PREDICTION_TIME, config->player_y + velocity.y * PREDICTION_TIME, config->player_z + velocity.z * PREDICTION_TIME }; return predicted; } ``` #### Level-of-Detail (LOD) Integration GRAPHITE supports hierarchical LOD systems where different quality versions of assets are stored in the same bundle: ```mermaid graph LR subgraph \"Distance-Based LOD\" A[High Quality<br/>0-50m] B[Medium Quality<br/>50-200m] C[Low Quality<br/>200-1000m] D[Impostor<br/>1000m+] end subgraph \"Performance-Based LOD\" E[GPU Load Monitor] F[Memory Pressure] G[Frame Rate Target] end subgraph \"GRAPHITE Bundle\" H[LOD 0 Assets] I[LOD 1 Assets] J[LOD 2 Assets] K[Impostor Data] end A --> H B --> I C --> J D --> K E --> A F --> B G --> C ``` ```c // LOD management system typedef struct { uint32_t lod_levels; float* distance_thresholds; float* quality_multipliers; bool enable_dynamic_lod; float target_frame_time_ms; } lod_config; const graphite_graph* select_appropriate_lod(const graphite_graph* asset_group, float distance, const lod_config* config) { // Static LOD selection based on distance uint32_t lod_level = 0; for (uint32_t i = 0; i < config->lod_levels; i++) { if (distance > config->distance_thresholds[i]) { lod_level = i + 1; } } // Dynamic LOD adjustment based on performance if (config->enable_dynamic_lod) { float current_frame_time = get_current_frame_time_ms(); if (current_frame_time > config->target_frame_time_ms * 1.2) { lod_level = min(lod_level + 1, config->lod_levels - 1); } else if (current_frame_time < config->target_frame_time_ms * 0.8) { lod_level = max(lod_level - 1, 0); } } // Find LOD asset in graph char lod_property[32]; snprintf(lod_property, sizeof(lod_property), \"lod_%u\", lod_level); return graphite_find_node_by_property(asset_group, \"lod_level\", lod_property); } ``` #### Hot Reload for Live Development Hot reload enables developers to see asset changes immediately without restarting the game: ```mermaid sequenceDiagram participant Dev as Developer participant FS as File System participant Watcher as File Watcher participant Builder as Asset Builder participant Game as Game Engine participant GRAPHITE as GRAPHITE Loader Dev->>FS: Modify texture.png FS->>Watcher: File changed event Watcher->>Builder: Trigger rebuild Builder->>Builder: Repack modified assets Builder->>GRAPHITE: Load new bundle GRAPHITE->>Game: Hot reload notification Game->>Game: Update texture in GPU Game->>Dev: Visual update visible ``` ```c // Hot reload implementation typedef struct { char* source_directory; char* bundle_path; void* file_watcher; pthread_t watcher_thread; bool enable_hot_reload; hot_reload_callback callback; void* user_data; } hot_reload_context; typedef void (*hot_reload_callback)(const char* asset_path, const graphite_graph* new_asset, void* user_data); bool setup_hot_reload(hot_reload_context* ctx, const char* source_dir, const char* bundle_path, hot_reload_callback callback) { ctx->source_directory = strdup(source_dir); ctx->bundle_path = strdup(bundle_path); ctx->callback = callback; ctx->enable_hot_reload = true; // Platform-specific file watching #ifdef _WIN32 ctx->file_watcher = setup_windows_directory_watcher(source_dir); #elif defined(__linux__) ctx->file_watcher = setup_inotify_watcher(source_dir); #elif defined(__APPLE__) ctx->file_watcher = setup_fsevents_watcher(source_dir); #endif // Start watcher thread pthread_create(&ctx->watcher_thread, NULL, file_watcher_thread, ctx); return ctx->file_watcher != NULL; } void* file_watcher_thread(void* arg) { hot_reload_context* ctx = (hot_reload_context*)arg; while (ctx->enable_hot_reload) { file_change_event event; if (wait_for_file_change(ctx->file_watcher, &event, 100)) { if (should_rebuild_for_change(&event)) { // Rebuild affected portion of bundle rebuild_bundle_incremental(ctx->source_directory, ctx->bundle_path, &event); // Reload and notify callback graphite_bundle* new_bundle = graphite_open(ctx->bundle_path); if (new_bundle) { const graphite_graph* asset = find_asset_by_path(new_bundle, event.file_path); if (asset && ctx->callback) { ctx->callback(event.file_path, asset, ctx->user_data); } } } } } return NULL; } ``` ### Content Delivery Networks (CDN) #### Progressive Download with Range Requests GRAPHITE bundles can be streamed from CDNs using HTTP range requests, allowing games to start playing before the entire bundle is downloaded: ```mermaid graph TB subgraph \"CDN Infrastructure\" A[Edge Server 1] B[Edge Server 2] C[Edge Server 3] D[Origin Server] end subgraph \"Client Side\" E[GRAPHITE Loader] F[Priority Queue] G[Range Request Manager] H[Local Cache] end subgraph \"Download Strategy\" I[Essential Assets First] J[Nearby World Data] K[Background Streaming] L[Predictive Prefetch] end D --> A D --> B D --> C E --> F F --> G G --> A G --> B G --> C E --> I I --> J J --> K K --> L A --> H B --> H C --> H ``` ```c // Progressive download implementation typedef struct { char* cdn_base_url; uint32_t max_concurrent_downloads; size_t initial_download_size; float download_timeout_seconds; bool enable_compression; } cdn_config; typedef struct { uint64_t start_offset; uint64_t end_offset; uint32_t priority; bool essential; download_callback callback; } range_request; bool download_bundle_progressive(const char* bundle_url, const cdn_config* config, const char* local_path) { // 1. Download header and chunk table first if (!download_range(bundle_url, 0, sizeof(graphite_file_header) + estimate_chunk_table_size(bundle_url), local_path)) { return false; } // 2. Parse header to understand bundle structure graphite_bundle* partial_bundle = graphite_open_partial(local_path); if (!partial_bundle) { return false; } // 3. Prioritize essential chunks range_request* requests = prioritize_chunk_downloads(partial_bundle); // 4. Download in priority order for (range_request* req = requests; req; req = req->next) { if (req->essential) { download_range_blocking(bundle_url, req->start_offset, req->end_offset, local_path); } else { download_range_async(bundle_url, req->start_offset, req->end_offset, local_path, req->callback); } } return true; } // Intelligent chunk prioritization range_request* prioritize_chunk_downloads(const graphite_bundle* bundle) { range_request* requests = NULL; // Priority 1: Root graph and string pool add_high_priority_chunk(&requests, bundle->header.root_graph_index); add_high_priority_chunk(&requests, bundle->header.string_pool_index); // Priority 2: Assets needed for immediate gameplay const graphite_graph* root = graphite_get_root_graph(bundle); for (uint32_t i = 0; i < graphite_get_node_count(root); i++) { const graphite_graph* node = graphite_get_node(root, i); if (is_essential_for_startup(node)) { add_high_priority_chunk(&requests, get_chunk_index_for_graph(node)); } } // Priority 3: World data near player spawn add_world_chunks_near_spawn(&requests, bundle); // Priority 4: Everything else add_remaining_chunks(&requests, bundle); return requests; } ``` #### Delta Updates and Patch Distribution GRAPHITE supports efficient delta updates that only download changed content: ```c // Delta update system typedef struct { uint32_t old_chunk_count; uint32_t new_chunk_count; chunk_delta* deltas; size_t total_delta_size; double compression_ratio; } bundle_delta; typedef struct { uint32_t chunk_index; enum { CHUNK_UNCHANGED, CHUNK_MODIFIED, CHUNK_ADDED, CHUNK_REMOVED } operation; size_t delta_size; void* delta_data; } chunk_delta; bundle_delta* create_bundle_delta(const char* old_bundle_path, const char* new_bundle_path) { graphite_bundle* old_bundle = graphite_open(old_bundle_path); graphite_bundle* new_bundle = graphite_open(new_bundle_path); if (!old_bundle || !new_bundle) { return NULL; } bundle_delta* delta = malloc(sizeof(bundle_delta)); delta->old_chunk_count = old_bundle->header.chunk_count; delta->new_chunk_count = new_bundle->header.chunk_count; delta->deltas = malloc(delta->new_chunk_count * sizeof(chunk_delta)); delta->total_delta_size = 0; // Compare chunks by content hash for (uint32_t i = 0; i < delta->new_chunk_count; i++) { const chunk_table_entry* new_entry = get_chunk_entry(new_bundle, i); const chunk_table_entry* old_entry = find_chunk_by_hash(old_bundle, new_entry->crc32); if (old_entry && chunks_identical(old_entry, new_entry)) { // Chunk unchanged delta->deltas[i].operation = CHUNK_UNCHANGED; delta->deltas[i].delta_size = 0; } else if (old_entry) { // Chunk modified - create binary delta void* old_data = get_chunk_data(old_bundle, old_entry - old_bundle->chunks); void* new_data = get_chunk_data(new_bundle, i); delta->deltas[i].operation = CHUNK_MODIFIED; delta->deltas[i].delta_data = create_binary_diff(old_data, old_entry->size, new_data, new_entry->size, &delta->deltas[i].delta_size); } else { // Chunk added delta->deltas[i].operation = CHUNK_ADDED; delta->deltas[i].delta_size = new_entry->size; delta->deltas[i].delta_data = get_chunk_data(new_bundle, i); } delta->total_delta_size += delta->deltas[i].delta_size; } // Calculate compression ratio size_t new_bundle_size = get_file_size(new_bundle_path); delta->compression_ratio = (double)delta->total_delta_size / new_bundle_size; graphite_close(old_bundle); graphite_close(new_bundle); return delta; } bool apply_bundle_delta(const char* old_bundle_path, const bundle_delta* delta, const char* output_path) { graphite_bundle* old_bundle = graphite_open(old_bundle_path); if (!old_bundle) { return false; } FILE* output = fopen(output_path, \"wb\"); if (!output) { graphite_close(old_bundle); return false; } // Write new header graphite_file_header new_header = old_bundle->header; new_header.chunk_count = delta->new_chunk_count; fwrite(&new_header, sizeof(new_header), 1, output); // Apply deltas chunk by chunk for (uint32_t i = 0; i < delta->new_chunk_count; i++) { switch (delta->deltas[i].operation) { case CHUNK_UNCHANGED: copy_chunk_from_old(old_bundle, i, output); break; case CHUNK_MODIFIED: apply_binary_diff(old_bundle, i, delta->deltas[i].delta_data, delta->deltas[i].delta_size, output); break; case CHUNK_ADDED: write_chunk_data(delta->deltas[i].delta_data, delta->deltas[i].delta_size, output); break; } } fclose(output); graphite_close(old_bundle); return true; } ``` ### Build Pipeline Integration #### Continuous Integration Workflows GRAPHITE integrates seamlessly with modern CI/CD pipelines: ```yaml # Example GitHub Actions workflow name: Asset Pipeline on: push: paths: - 'assets/**' pull_request: paths: - 'assets/**' jobs: build-assets: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 with: lfs: true - name: Install GRAPHITE CLI run: | curl -fsSL https://graphite.dev/install.sh | sh echo \"$HOME/.graphite/bin\" >> $GITHUB_PATH - name: Validate asset inputs run: graphite validate-inputs assets/ - name: Train compression dictionary run: | graphite train-dict --pattern \"*.json\" --pattern \"*.xml\" \\ --size 64KB assets/ dict.zstd - name: Build asset bundle run: | graphite pack --compress --level 5 \\ --integrity --mandatory \\ --dictionary dict.zstd \\ --set \"build_id=$GITHUB_RUN_ID\" \\ --set \"commit_sha=$GITHUB_SHA\" \\ --set \"build_date=$(date -u +%Y-%m-%dT%H:%M:%SZ)\" \\ assets/ dist/game_assets.graphite - name: Verify bundle integrity run: graphite verify --comprehensive dist/game_assets.graphite - name: Run performance benchmarks run: | graphite bench --format json dist/game_assets.graphite > benchmark.json # Check for performance regressions if [ -f baseline_benchmark.json ]; then graphite perf-check --baseline baseline_benchmark.json \\ --current benchmark.json \\ --max-load-time-increase 10% fi - name: Generate analysis report run: | graphite analyze --recommendations dist/game_assets.graphite > analysis.md echo \"## Bundle Analysis\" >> $GITHUB_STEP_SUMMARY cat analysis.md >> $GITHUB_STEP_SUMMARY - name: Upload artifacts uses: actions/upload-artifact@v3 with: name: game-assets path: | dist/game_assets.graphite benchmark.json analysis.md - name: Upload to CDN (production only) if: github.ref == 'refs/heads/main' run: | aws s3 cp dist/game_assets.graphite \\ s3://game-assets-cdn/v${GITHUB_RUN_ID}/game_assets.graphite \\ --metadata \"commit-sha=$GITHUB_SHA,build-date=$(date -u +%Y-%m-%dT%H:%M:%SZ)\" env: AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }} AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }} ``` #### Asset Processing Pipelines Complex asset processing workflows can be defined using GRAPHITE's pipeline system: ```yaml # asset_pipeline.yaml name: Game Asset Pipeline version: 1.0 variables: compression_level: 5 target_platforms: [pc, mobile, console] quality_presets: pc: high mobile: medium console: high stages: - name: validation description: Validate input assets steps: - validate_formats - check_naming_conventions - verify_metadata - name: preprocessing description: Preprocess assets for optimization parallel: true steps: - texture_optimization: input_patterns: [\"*.png\", \"*.jpg\", \"*.tga\"] operations: - resize_for_platform - convert_to_optimal_format - generate_mipmaps - audio_processing: input_patterns: [\"*.wav\", \"*.aiff\"] operations: - normalize_volume - convert_to_ogg - generate_platform_variants - mesh_optimization: input_patterns: [\"*.fbx\", \"*.obj\"] operations: - optimize_geometry - generate_lods - bake_transforms - name: bundling description: Create platform-specific bundles parallel: true matrix: platform: ${target_platforms} steps: - create_bundle: input: preprocessed/${platform}/ output: dist/${platform}/assets.graphite compression: level: ${compression_level} dictionary: true integrity: enable: true mandatory: true metadata: platform: ${platform} quality: ${quality_presets[platform]} build_id: ${BUILD_ID} - name: verification description: Verify bundle quality depends_on: bundling steps: - integrity_check - performance_benchmark - size_regression_check - quality_gate_validation - name: deployment description: Deploy to distribution networks condition: branch == 'main' depends_on: verification steps: - upload_to_cdn - update_manifest - notify_clients post_processing: - generate_reports - cleanup_temp_files - send_notifications ``` ```c // Pipeline execution engine typedef struct { char* name; char* description; pipeline_step* steps; uint32_t step_count; bool parallel; char** depends_on; uint32_t dependency_count; char* condition; } pipeline_stage; typedef struct { char* name; step_operation* operations; uint32_t operation_count; char** input_patterns; uint32_t pattern_count; pipeline_context* context; } pipeline_step; bool execute_pipeline(const char* pipeline_file, const pipeline_context* context) { pipeline_config* config = parse_pipeline_yaml(pipeline_file); if (!config) { return false; } // Build dependency graph dependency_graph* graph = build_dependency_graph(config); // Execute stages in topological order pipeline_stage** execution_order = topological_sort(graph); for (pipeline_stage** stage = execution_order; *stage; stage++) { if (!evaluate_condition((*stage)->condition, context)) { continue; } if ((*stage)->parallel) { execute_stage_parallel(*stage, context); } else { execute_stage_sequential(*stage, context); } if (context->error_occurred) { return false; } } return true; } void execute_stage_parallel(const pipeline_stage* stage, const pipeline_context* context) { thread_pool* pool = create_thread_pool(get_cpu_count()); for (uint32_t i = 0; i < stage->step_count; i++) { pipeline_task* task = create_pipeline_task(&stage->steps[i], context); queue_task(pool, task); } wait_for_completion(pool); destroy_thread_pool(pool); } ``` --- ## Implementation Guidelines ### Platform-Specific Considerations #### Linux Implementation GRAPHITE on Linux leverages advanced kernel features for optimal performance: ```c // Linux-specific optimizations #ifdef __linux__ // Use io_uring for high-performance async I/O bool setup_linux_async_io(graphite_bundle* bundle) { struct io_uring* ring = &bundle->io_ring; // Initialize with advanced features struct io_uring_params params = {0}; params.flags = IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL; params.sq_thread_idle = 2000; // 2 second idle time int ret = io_uring_queue_init_params(256, ring, &params); if (ret < 0) { // Fallback to basic io_uring ret = io_uring_queue_init(64, ring, 0); } return ret >= 0; } // Enable huge pages for large bundles bool enable_huge_pages(void* addr, size_t size) { // Check if transparent huge pages are available int fd = open(\"/sys/kernel/mm/transparent_hugepage/enabled\", O_RDONLY); if (fd >= 0) { char buffer[64]; read(fd, buffer, sizeof(buffer)); close(fd); if (strstr(buffer, \"[always]\") || strstr(buffer, \"[madvise]\")) { return madvise(addr, size, MADV_HUGEPAGE) == 0; } } return false; } // NUMA awareness bool setup_numa_optimizations(graphite_bundle* bundle) { if (numa_available() < 0) { return false; } int node_count = numa_num_configured_nodes(); int current_node = numa_node_of_cpu(sched_getcpu()); // Allocate bundle arena on current NUMA node size_t arena_size = bundle->arena_size; void* numa_arena = numa_alloc_onnode(arena_size, current_node); if (numa_arena) { // Migrate existing arena if needed memcpy(numa_arena, bundle->arena, arena_size); numa_free(bundle->arena, arena_size); bundle->arena = numa_arena; // Set memory policy for future allocations numa_set_preferred(current_node); return true; } return false; } #endif // __linux__ ``` #### Windows Implementation Windows-specific optimizations focus on IOCP and memory management: ```c // Windows-specific optimizations #ifdef _WIN32 // Use I/O Completion Ports for async operations bool setup_windows_async_io(graphite_bundle* bundle) { HANDLE completion_port = CreateIoCompletionPort(INVALID_HANDLE_VALUE, NULL, 0, 0); if (completion_port == NULL) { return false; } bundle->completion_port = completion_port; // Associate file handle with completion port HANDLE file_handle = CreateFileA(bundle->file_path, GENERIC_READ, FILE_SHARE_READ, NULL, OPEN_EXISTING, FILE_FLAG_OVERLAPPED | FILE_FLAG_NO_BUFFERING, NULL); if (file_handle == INVALID_HANDLE_VALUE) { CloseHandle(completion_port); return false; } bundle->file_handle = file_handle; // Associate with completion port if (CreateIoCompletionPort(file_handle, completion_port, 0, 0) == NULL) { CloseHandle(file_handle); CloseHandle(completion_port); return false; } return true; } // Enable large pages on Windows bool enable_large_pages_windows(void* addr, size_t size) { // Get large page minimum SIZE_T large_page_min = GetLargePageMinimum(); if (large_page_min == 0 || size < large_page_min) { return false; } // Check privileges HANDLE token; if (!OpenProcessToken(GetCurrentProcess(), TOKEN_ADJUST_PRIVILEGES, &token)) { return false; } TOKEN_PRIVILEGES tp = {0}; tp.PrivilegeCount = 1; tp.Privileges[0].Attributes = SE_PRIVILEGE_ENABLED; if (!LookupPrivilegeValue(NULL, SE_LOCK_MEMORY_NAME, &tp.Privileges[0].Luid)) { CloseHandle(token); return false; } bool success = AdjustTokenPrivileges(token, FALSE, &tp, 0, NULL, NULL); CloseHandle(token); if (!success) { return false; } // Allocate with large pages void* large_mem = VirtualAlloc(NULL, size, MEM_COMMIT | MEM_RESERVE | MEM_LARGE_PAGES, PAGE_READWRITE); if (large_mem) { // Copy existing data memcpy(large_mem, addr, size); VirtualFree(addr, 0, MEM_RELEASE); addr = large_mem; return true; } return false; } // Windows memory mapping with section objects HANDLE create_memory_section(const char* file_path, size_t* file_size) { HANDLE file_handle = CreateFileA(file_path, GENERIC_READ, FILE_SHARE_READ, NULL, OPEN_EXISTING, FILE_ATTRIBUTE_NORMAL, NULL); if (file_handle == INVALID_HANDLE_VALUE) { return NULL; } LARGE_INTEGER size; if (!GetFileSizeEx(file_handle, &size)) { CloseHandle(file_handle); return NULL; } *file_size = size.QuadPart; HANDLE section = CreateFileMapping(file_handle, NULL, PAGE_READONLY, size.HighPart, size.LowPart, NULL); CloseHandle(file_handle); return section; } #endif // _WIN32 ``` #### macOS Implementation macOS optimizations leverage unified memory and Metal integration: ```c // macOS-specific optimizations #ifdef __APPLE__ #include <sys/mman.h> #include <mach/mach.h> #include <IOKit/IOKitLib.h> // Optimize for Apple Silicon unified memory bool setup_macos_unified_memory(graphite_bundle* bundle) { // Check if running on Apple Silicon size_t size = sizeof(int); int apple_silicon = 0; sysctlbyname(\"hw.optional.arm64\", &apple_silicon, &size, NULL, 0); if (apple_silicon) { // On Apple Silicon, prefer memory-mapped files with specific flags int fd = open(bundle->file_path, O_RDONLY); if (fd >= 0) { void* mapped = mmap(NULL, bundle->file_size, PROT_READ, MAP_PRIVATE | MAP_NOCACHE, fd, 0); if (mapped != MAP_FAILED) { // Advise kernel about access patterns madvise(mapped, bundle->file_size, MADV_SEQUENTIAL); bundle->mapped_memory = mapped; close(fd); return true; } close(fd); } } return false; } // Metal integration for GPU assets bool setup_metal_integration(graphite_bundle* bundle) { // Create Metal device id<MTLDevice> device = MTLCreateSystemDefaultDevice(); if (!device) { return false; } bundle->metal_device = device; // Create command queue for GPU operations id<MTLCommandQueue> queue = [device newCommandQueue]; bundle->metal_queue = queue; return true; } // Optimize for macOS file system bool optimize_for_apfs(graphite_bundle* bundle) { // Use F_NOCACHE to avoid polluting buffer cache for large files int fd = open(bundle->file_path, O_RDONLY); if (fd >= 0) { fcntl(fd, F_NOCACHE, 1); // Use F_RDADVISE for sequential access hint struct radvisory advice = { .ra_offset = 0, .ra_count = bundle->file_size }; fcntl(fd, F_RDADVISE, &advice); close(fd); return true; } return false; } #endif // __APPLE__ ``` ### Error Handling and Resilience #### Comprehensive Error Recovery ```c // Robust error handling system typedef enum { RECOVERY_STRATEGY_RETRY, RECOVERY_STRATEGY_FALLBACK, RECOVERY_STRATEGY_PARTIAL_LOAD, RECOVERY_STRATEGY_FAIL_SAFE, RECOVERY_STRATEGY_ABORT } recovery_strategy; typedef struct { graphite_error error_code; recovery_strategy strategy; uint32_t max_retries; uint32_t retry_delay_ms; bool (*fallback_handler)(void* context); void* context; } error_recovery_config; // Error recovery system bool handle_graphite_error(graphite_error error, const error_recovery_config* config) { static uint32_t retry_count = 0; switch (config->strategy) { case RECOVERY_STRATEGY_RETRY: if (retry_count < config->max_retries) { retry_count++; // Exponential backoff uint32_t delay = config->retry_delay_ms * (1 << (retry_count - 1)); platform_sleep(delay); return true; // Retry the operation } break; case RECOVERY_STRATEGY_FALLBACK: if (config->fallback_handler) { return config->fallback_handler(config->context); } break; case RECOVERY_STRATEGY_PARTIAL_LOAD: return attempt_partial_bundle_load(config->context); case RECOVERY_STRATEGY_FAIL_SAFE: return load_minimal_bundle(config->context); case RECOVERY_STRATEGY_ABORT: default: return false; } return false; } // Partial loading for corrupted bundles bool attempt_partial_bundle_load(graphite_bundle* bundle) { // Try to load whatever chunks are valid uint32_t valid_chunks = 0; for (uint32_t i = 0; i < bundle->header.chunk_count; i++) { if (verify_chunk_crc32(bundle, i)) { mark_chunk_valid(bundle, i); valid_chunks++; } else { mark_chunk_invalid(bundle, i); } } if (valid_chunks > 0) { // Build partial graph with only valid chunks return rebuild_bundle_graph_partial(bundle); } return false; } // Fail-safe minimal bundle bool load_minimal_bundle(void* context) { // Load absolutely essential assets only const char* essential_assets[] = { \"fallback_texture.png\", \"default_mesh.obj\", \"error_sound.wav\" }; // Create minimal in-memory bundle return create_emergency_bundle(essential_assets, sizeof(essential_assets) / sizeof(char*)); } ``` #### Memory Safety and Bounds Checking ```c // Safe memory access wrappers static inline bool validate_offset(const graphite_bundle* bundle, uint64_t offset, uint64_t size) { if (offset > bundle->file_size) { return false; } if (size > bundle->file_size - offset) { return false; } return true; } // Safe chunk access const void* safe_get_chunk_data(const graphite_bundle* bundle, uint32_t chunk_index, size_t* size) { if (chunk_index >= bundle->header.chunk_count) { *size = 0; return NULL; } const chunk_table_entry* entry = &bundle->chunks[chunk_index]; if (!validate_offset(bundle, entry->offset, entry->size)) { *size = 0; return NULL; } *size = entry->size; return (const uint8_t*)bundle->mapped_memory + entry->offset; } // Safe string access with bounds checking const char* safe_get_string(const graphite_bundle* bundle, uint32_t string_id) { if (!bundle->string_pool || string_id >= bundle->string_count) { return \"\"; } size_t string_offset = bundle->string_offsets[string_id]; if (string_offset >= bundle->string_pool_size) { return \"\"; } const char* string_ptr = (const char*)bundle->string_pool + string_offset; // Ensure string is null-terminated within bounds size_t max_length = bundle->string_pool_size - string_offset; if (strnlen(string_ptr, max_length) == max_length) { return \"\"; // String not properly terminated } return string_ptr; } // Integer overflow protection bool safe_multiply_size_t(size_t a, size_t b, size_t* result) { if (a == 0 || b == 0) { *result = 0; return true; } if (a > SIZE_MAX / b) { return false; // Overflow would occur } *result = a * b; return true; } bool safe_add_size_t(size_t a, size_t b, size_t* result) { if (a > SIZE_MAX - b) { return false; // Overflow would occur } *result = a + b; return true; } ``` ### Memory Management Best Practices #### Arena Allocation Strategies ```c // Advanced arena allocation with multiple pools typedef struct { void* base_address; size_t total_size; size_t used_size; uint32_t allocation_count; // Multiple allocation strategies linear_allocator linear; stack_allocator stack; pool_allocator pools[8]; // Different sized pools // Memory tracking allocation_tracker tracker; memory_stats stats; } advanced_arena; // Create optimized arena based on bundle analysis advanced_arena* create_optimized_arena(const graphite_bundle* bundle) { // Analyze bundle to determine optimal allocation strategy bundle_memory_analysis analysis = analyze_memory_requirements(bundle); size_t arena_size = calculate_optimal_arena_size(&analysis); void* arena_memory = allocate_aligned_memory(arena_size, 64); if (!arena_memory) { return NULL; } advanced_arena* arena = malloc(sizeof(advanced_arena)); arena->base_address = arena_memory; arena->total_size = arena_size; arena->used_size = 0; arena->allocation_count = 0; // Initialize sub-allocators based on analysis init_linear_allocator(&arena->linear, arena_memory, analysis.linear_allocation_size); void* stack_memory = (uint8_t*)arena_memory + analysis.linear_allocation_size; init_stack_allocator(&arena->stack, stack_memory, analysis.stack_allocation_size); // Initialize pools for different object sizes void* pool_memory = (uint8_t*)stack_memory + analysis.stack_allocation_size; init_size_pools(arena->pools, pool_memory, &analysis); return arena; } // Pool allocator for frequent small allocations typedef struct { void* memory_blocks; uint32_t block_size; uint32_t block_count; uint32_t free_blocks; uint32_t* free_list; } pool_allocator; void* pool_allocate(pool_allocator* pool) { if (pool->free_blocks == 0) { return NULL; // Pool exhausted } uint32_t block_index = pool->free_list[--pool->free_blocks]; return (uint8_t*)pool->memory_blocks + (block_index * pool->block_size); } void pool_deallocate(pool_allocator* pool, void* ptr) { if (!ptr) return; uintptr_t block_addr = (uintptr_t)ptr; uintptr_t base_addr = (uintptr_t)pool->memory_blocks; if (block_addr < base_addr || block_addr >= base_addr + (pool->block_count * pool->block_size)) { return; // Invalid pointer } uint32_t block_index = (block_addr - base_addr) / pool->block_size; if (pool->free_blocks < pool->block_count) { pool->free_list[pool->free_blocks++] = block_index; } } ``` #### Memory Debugging and Profiling ```c // Memory debugging infrastructure #ifdef GRAPHITE_DEBUG_MEMORY typedef struct allocation_record { void* address; size_t size; const char* file; int line; const char* function; uint64_t timestamp; uint32_t allocation_id; struct allocation_record* next; } allocation_record; static allocation_record* g_allocations = NULL; static uint32_t g_allocation_counter = 0; static pthread_mutex_t g_allocation_mutex = PTHREAD_MUTEX_INITIALIZER; void* debug_malloc(size_t size, const char* file, int line, const char* func) { void* ptr = malloc(size + sizeof(size_t) + 8); // Add padding for overflow detection if (!ptr) return NULL; // Store size at beginning *(size_t*)ptr = size; // Add magic numbers for overflow detection uint32_t* magic_start = (uint32_t*)((uint8_t*)ptr + sizeof(size_t)); uint32_t* magic_end = (uint32_t*)((uint8_t*)ptr + sizeof(size_t) + size + 4); *magic_start = 0xDEADBEEF; *magic_end = 0xBEEFDEAD; void* user_ptr = (uint8_t*)ptr + sizeof(size_t) + 4; // Record allocation pthread_mutex_lock(&g_allocation_mutex); allocation_record* record = malloc(sizeof(allocation_record)); record->address = user_ptr; record->size = size; record->file = file; record->line = line; record->function = func; record->timestamp = get_timestamp_ns(); record->allocation_id = ++g_allocation_counter; record->next = g_allocations; g_allocations = record; pthread_mutex_unlock(&g_allocation_mutex); return user_ptr; } void debug_free(void* ptr, const char* file, int line, const char* func) { if (!ptr) return; void* real_ptr = (uint8_t*)ptr - sizeof(size_t) - 4; size_t size = *(size_t*)real_ptr; // Check for buffer overflows uint32_t* magic_start = (uint32_t*)((uint8_t*)real_ptr + sizeof(size_t)); uint32_t* magic_end = (uint32_t*)((uint8_t*)ptr + size); if (*magic_start != 0xDEADBEEF) { fprintf(stderr, \"MEMORY CORRUPTION: Start magic corrupted at %p (allocated at %s:%d)\ \", ptr, file, line); abort(); } if (*magic_end != 0xBEEFDEAD) { fprintf(stderr, \"MEMORY CORRUPTION: End magic corrupted at %p (allocated at %s:%d)\ \", ptr, file, line); abort(); } // Remove from allocation list pthread_mutex_lock(&g_allocation_mutex); allocation_record** current = &g_allocations; while (*current) { if ((*current)->address == ptr) { allocation_record* to_remove = *current; *current = (*current)->next; free(to_remove); break; } current = &(*current)->next; } pthread_mutex_unlock(&g_allocation_mutex); free(real_ptr); } #define graphite_malloc(size) debug_malloc(size, __FILE__, __LINE__, __FUNCTION__) #define graphite_free(ptr) debug_free(ptr, __FILE__, __LINE__, __FUNCTION__) #else #define graphite_malloc(size) malloc(size) #define graphite_free(ptr) free(ptr) #endif // GRAPHITE_DEBUG_MEMORY ``` --- ## Security Model ### Threat Analysis and Mitigation #### Attack Surface Analysis GRAPHITE's attack surface includes file format parsing, memory management, and cryptographic operations: ```mermaid graph TB subgraph \"Attack Vectors\" A[Malicious Bundle Files] B[Network Interception] C[Memory Corruption] D[Side Channel Attacks] E[Supply Chain Attacks] end subgraph \"Defense Layers\" F[Input Validation] G[Cryptographic Verification] H[Memory Safety] I[Secure Coding Practices] J[Fuzzing & Testing] end subgraph \"Security Controls\" K[Format Validation] L[Bounds Checking] M[BLAKE3 Hash Trees] N[CRC32 Verification] O[Arena Isolation] end A --> F B --> G C --> H D --> I E --> J F --> K F --> L G --> M G --> N H --> O ``` #### Secure File Format Design ```c // Security-first format validation typedef struct { uint64_t max_file_size; uint32_t max_chunk_count; uint32_t max_graph_depth; uint32_t max_string_length; bool require_integrity_tree; bool require_mandatory_verification; double max_compression_ratio; } security_policy; // Comprehensive input validation bool validate_bundle_security(const graphite_bundle* bundle, const security_policy* policy) { // 1. File size limits if (bundle->header.file_size > policy->max_file_size) { return false; } // 2. Chunk count limits (prevent memory exhaustion) if (bundle->header.chunk_count > policy->max_chunk_count) { return false; } // 3. Validate all chunk table entries for (uint32_t i = 0; i < bundle->header.chunk_count; i++) { const chunk_table_entry* entry = &bundle->chunks[i]; // Check for integer overflow in offset + size if (entry->offset > UINT64_MAX - entry->size) { return false; } // Ensure chunk doesn't extend beyond file if (entry->offset + entry->size > bundle->header.file_size) { return false; } // Validate chunk alignment if (entry->offset % 8 != 0) { return false; } } // 4. Integrity requirements if (policy->require_integrity_tree && bundle->header.integrity_root_index == 0) { return false; } if (policy->require_mandatory_verification && !(bundle->header.file_flags & FLAG_MANDATORY_INTEGRITY_CHECK)) { return false; } // 5. Graph structure validation return validate_graph_structure_security(bundle, policy); } bool validate_graph_structure_security(const graphite_bundle* bundle, const security_policy* policy) { // Prevent graph bombs (deeply nested or cyclic structures) for (uint32_t i = 0; i < bundle->header.chunk_count; i++) { const chunk_table_entry* entry = &bundle->chunks[i]; if (entry->kind == CHUNK_GRAPH) { if (!validate_graph_chunk_security(bundle, i, policy, 0)) { return false; } } } return true; } bool validate_graph_chunk_security(const graphite_bundle* bundle, uint32_t chunk_index, const security_policy* policy, uint32_t depth) { // Prevent stack overflow from deep recursion if (depth > policy->max_graph_depth) { return false; } const graph_chunk_header* header = get_graph_header(bundle, chunk_index); if (!header) { return false; } // Validate node count doesn't cause integer overflow size_t node_table_size; if (!safe_multiply_size_t(header->node_count, sizeof(uint32_t), &node_table_size)) { return false; } // Validate edge count doesn't cause integer overflow size_t edge_table_size; if (!safe_multiply_size_t(header->edge_count, sizeof(edge_descriptor), &edge_table_size)) { return false; } // Recursively validate child graphs const uint32_t* node_table = get_node_table(bundle, chunk_index); for (uint32_t i = 0; i < header->node_count; i++) { uint32_t child_chunk = node_table[i]; if (child_chunk < bundle->header.chunk_count) { const chunk_table_entry* child_entry = &bundle->chunks[child_chunk]; if (child_entry->kind == CHUNK_GRAPH) { if (!validate_graph_chunk_security(bundle, child_chunk, policy, depth + 1)) { return false; } } } } return true; } ``` #### Cryptographic Security Implementation ```c // Secure BLAKE3 implementation with timing attack protection typedef struct { blake3_hasher hasher; uint8_t key[32]; bool use_keyed_hash; uint64_t hash_counter; } secure_hasher; // Initialize secure hasher with optional key bool secure_hasher_init(secure_hasher* hasher, const uint8_t* key) { if (key) { blake3_hasher_init_keyed(&hasher->hasher, key); memcpy(hasher->key, key, 32); hasher->use_keyed_hash = true; } else { blake3_hasher_init(&hasher->hasher); hasher->use_keyed_hash = false; } hasher->hash_counter = 0; return true; } // Update hash with timing attack protection void secure_hasher_update(secure_hasher* hasher, const void* data, size_t size) { // Add padding to prevent timing attacks based on data size const size_t padding_granularity = 1024; size_t padded_size = ((size + padding_granularity - 1) / padding_granularity) * padding_granularity; blake3_hasher_update(&hasher->hasher, data, size); // Hash padding bytes if needed if (padded_size > size) { static const uint8_t padding[1024] = {0}; blake3_hasher_update(&hasher->hasher, padding, padded_size - size); } hasher->hash_counter++; } // Finalize hash with constant-time comparison bool secure_hasher_finalize(secure_hasher* hasher, uint8_t* output, const uint8_t* expected_hash) { blake3_hasher_finalize(&hasher->hasher, output, 32); if (expected_hash) { // Constant-time comparison to prevent timing attacks return crypto_verify_32(output, expected_hash) == 0; } return true; } // Constant-time memory comparison int crypto_verify_32(const uint8_t* a, const uint8_t* b) { uint8_t result = 0; for (int i = 0; i < 32; i++) { result |= a[i] ^ b[i]; } return (1 & ((result - 1) >> 8)) - 1; } ``` ### Secure Development Practices #### Fuzzing Infrastructure ```c // LibFuzzer integration for security testing #ifdef FUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION // Fuzzer entry point int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) { // Minimum size check if (size < sizeof(graphite_file_header)) { return 0; } // Create temporary file from fuzz input char temp_path[] = \"/tmp/fuzz_bundle_XXXXXX\"; int fd = mkstemp(temp_path); if (fd == -1) { return 0; } write(fd, data, size); close(fd); // Test GRAPHITE loading - should never crash graphite_bundle* bundle = graphite_open(temp_path); if (bundle) { // Exercise various operations fuzz_exercise_bundle(bundle); graphite_close(bundle); } unlink(temp_path); return 0; } void fuzz_exercise_bundle(graphite_bundle* bundle) { // Test root graph access const graphite_graph* root = graphite_get_root_graph(bundle); if (!root) return; // Test node iteration (with limits to prevent infinite loops) uint32_t node_count = graphite_get_node_count(root); node_count = min(node_count, 1000); // Limit for fuzzing for (uint32_t i = 0; i < node_count; i++) { const graphite_graph* node = graphite_get_node(root, i); if (node) { // Test property access uint32_t prop_count = graphite_get_property_count(node); prop_count = min(prop_count, 100); // Limit for fuzzing for (uint32_t j = 0; j < prop_count; j++) { graphite_property prop = graphite_get_property_by_index(node, j); // Properties are just accessed, not used (void)prop; } } } // Test edge iteration uint32_t edge_count = graphite_get_edge_count(root); edge_count = min(edge_count, 1000); // Limit for fuzzing for (uint32_t i = 0; i < edge_count; i++) { graphite_edge edge = graphite_get_edge(root, i); // Edge is just accessed, not used (void)edge; } // Test string pool access for (uint32_t i = 0; i < 100; i++) { // Test first 100 strings const char* str = graphite_get_string(bundle, i); if (!str) break; // String is just accessed, not used (void)str; } } // Address Sanitizer configuration __attribute__((no_sanitize(\"address\"))) void* fuzz_malloc(size_t size) { // Custom allocator for fuzzing that detects overflows void* ptr = malloc(size + 32); // Add guard pages if (ptr) { memset((uint8_t*)ptr + size, 0xDE, 32); // Guard pattern } return ptr; } #endif // FUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION ``` #### Static Analysis Integration ```c // Static analysis annotations for security #ifdef __clang_analyzer__ #define SECURITY_SENSITIVE __attribute__((analyzer_noreturn)) #define TAINTED_INPUT __attribute__((analyzer_taint_source)) #define SANITIZED __attribute__((analyzer_taint_sink)) #else #define SECURITY_SENSITIVE #define TAINTED_INPUT #define SANITIZED #endif // Taint tracking for untrusted input TAINTED_INPUT const void* get_untrusted_chunk_data(const graphite_bundle* bundle, uint32_t chunk_index) { // Mark this data as coming from untrusted source return get_chunk_data(bundle, chunk_index); } SANITIZED bool validate_and_sanitize_chunk(const void* tainted_data, size_t size, void** clean_data, size_t* clean_size) { // Comprehensive validation of untrusted data if (!tainted_data || size == 0) { return false; } // Size limits if (size > MAX_SAFE_CHUNK_SIZE) { return false; } // Content validation based on expected format if (!validate_chunk_format(tainted_data, size)) { return false; } // Copy to clean memory *clean_data = malloc(size); if (!*clean_data) { return false; } memcpy(*clean_data, tainted_data, size); *clean_size = size; return true; } // Security-critical function that should never return SECURITY_SENSITIVE void security_violation_detected(const char* violation_type, const char* file, int line) { // Log security violation syslog(LOG_CRIT, \"SECURITY VIOLATION: %s at %s:%d\", violation_type, file, line); // Immediately terminate the process abort(); } ``` --- ## Performance Benchmarks ### Reference Performance Targets GRAPHITE is designed to meet aggressive performance targets across different hardware configurations: #### Desktop Performance (8-core Intel i7-10700K, 32GB DDR4-3200, NVMe SSD) | Bundle Size | Load Time | Memory Usage | Task Latency (P99) | |-------------|-----------|--------------|-------------------| | 10 MB | <15 ms | <8 MB | <0.5 ms | | 100 MB | <45 ms | <32 MB | <1.0 ms | | 1 GB | <180 ms | <160 MB | <2.5 ms | | 10 GB | <1.8 s | <1.2 GB | <3.5 ms | #### Mobile Performance (Apple A15 Bionic, 6GB RAM, UFS 3.1) | Bundle Size | Load Time | Memory Usage | Battery Impact | |-------------|-----------|--------------|----------------| | 10 MB | <25 ms | <12 MB | <0.1% | | 100 MB | <85 ms | <48 MB | <0.3% | | 500 MB | <420 ms | <180 MB | <1.2% | | 2 GB | <1.9 s | <480 MB | <3.8% | #### Console Performance (PS5/Xbox Series X equivalent) | Bundle Size | Load Time | Memory Usage | GPU Transfer | |-------------|-----------|--------------|--------------| | 100 MB | <35 ms | <24 MB | <15 ms | | 1 GB | <150 ms | <128 MB | <85 ms | | 5 GB | <750 ms | <512 MB | <380 ms | | 20 GB | <3.2 s | <1.8 GB | <1.5 s | ### Detailed Performance Analysis #### Load Time Breakdown ```mermaid pie title Load Time Distribution (1GB Bundle) \"File I/O\" : 25 \"CRC Verification\" : 20 \"Decompression\" : 35 \"Hash Verification\" : 12 \"Pointer Hydration\" : 8 ``` #### Memory Usage Patterns ```mermaid graph LR subgraph \"Memory Timeline\" A[Initial: 0 MB] --> B[Header: 1 MB] B --> C[Arena: 160 MB] C --> D[Peak: 180 MB] D --> E[Steady: 165 MB] end subgraph \"Memory Types\" F[Read-Only Mapping] G[Arena Allocation] H[Decompression Buffers] I[Hash Verification] end ``` #### Scalability Analysis ```c // Performance scaling measurement typedef struct { size_t bundle_size; uint64_t load_time_ns; size_t memory_usage; uint64_t task_latency_p99_ns; double cpu_utilization; double io_bandwidth_mbps; } performance_measurement; // Comprehensive benchmark suite void run_performance_benchmarks(void) { const size_t test_sizes[] = { 10 * 1024 * 1024, // 10 MB 100 * 1024 * 1024, // 100 MB 1024 * 1024 * 1024, // 1 GB 10LL * 1024 * 1024 * 1024 // 10 GB }; printf(\"GRAPHITE Performance Benchmark Suite\ \"); printf(\"=====================================\ \ \"); for (size_t i = 0; i < sizeof(test_sizes) / sizeof(test_sizes[0]); i++) { performance_measurement result = benchmark_bundle_size(test_sizes[i]); print_performance_result(&result); } // Platform-specific optimizations benchmark benchmark_platform_optimizations(); // Stress testing benchmark_stress_conditions(); // Regression testing benchmark_regression_suite(); } performance_measurement benchmark_bundle_size(size_t target_size) { performance_measurement result = {0}; result.bundle_size = target_size; // Create test bundle of target size char* test_bundle_path = create_test_bundle(target_size); // Warm up system for (int i = 0; i < 3; i++) { graphite_bundle* bundle = graphite_open(test_bundle_path); if (bundle) { graphite_close(bundle); } } // Measure load performance uint64_t start_time = get_timestamp_ns(); size_t peak_memory = get_current_memory_usage(); graphite_performance_stats stats; graphite_bundle* bundle = graphite_open_ex(test_bundle_path, GRAPHITE_VERIFY_HASHES | GRAPHITE_DECOMPRESS | GRAPHITE_HW_ACCELERATION, &stats); uint64_t end_time = get_timestamp_ns(); size_t final_memory = get_current_memory_usage(); if (bundle) { result.load_time_ns = end_time - start_time; result.memory_usage = final_memory > peak_memory ? final_memory - peak_memory : 0; // Measure task latency result.task_latency_p99_ns = measure_task_latency_p99(bundle); // Measure CPU utilization during operation result.cpu_utilization = measure_cpu_utilization(bundle); // Measure I/O bandwidth result.io_bandwidth_mbps = (double)target_size / (result.load_time_ns / 1e9) / (1024 * 1024); graphite_close(bundle); } cleanup_test_bundle(test_bundle_path); return result; } // Task latency measurement uint64_t measure_task_latency_p99(graphite_bundle* bundle) { const int num_samples = 10000; uint64_t* latencies = malloc(num_samples * sizeof(uint64_t)); const graphite_graph* root = graphite_get_root_graph(bundle); uint32_t node_count = graphite_get_node_count(root); // Measure random access latency for (int i = 0; i < num_samples; i++) { uint32_t random_index = rand() % node_count; uint64_t start = get_timestamp_ns(); const graphite_graph* node = graphite_get_node(root, random_index); uint64_t end = get_timestamp_ns(); latencies[i] = end - start; // Use the node to prevent optimization volatile const void* ptr = node; (void)ptr; } // Sort and find P99 qsort(latencies, num_samples, sizeof(uint64_t), compare_uint64); uint64_t p99_latency = latencies[(int)(num_samples * 0.99)]; free(latencies); return p99_latency; } ``` ### Performance Optimization Strategies #### CPU Architecture Optimizations ```c // CPU feature detection and optimization typedef struct { bool has_sse42; bool has_avx2; bool has_bmi2; bool has_aes_ni; bool has_arm_crc32; bool has_arm_neon; int cache_line_size; int l1_cache_size; int l2_cache_size; int l3_cache_size; } cpu_features; cpu_features detect_cpu_features(void) { cpu_features features = {0}; #if defined(__x86_64__) || defined(_M_X64) // x86_64 feature detection uint32_t eax, ebx, ecx, edx; // CPUID level 1 __cpuid_count(1, 0, eax, ebx, ecx, edx); features.has_sse42 = (ecx & (1 << 20)) != 0; features.has_aes_ni = (ecx & (1 << 25)) != 0; // CPUID level 7 __cpuid_count(7, 0, eax, ebx, ecx, edx); features.has_avx2 = (ebx & (1 << 5)) != 0; features.has_bmi2 = (ebx & (1 << 8)) != 0; #elif defined(__aarch64__) // ARM64 feature detection features.has_arm_crc32 = check_arm_hwcap(HWCAP_CRC32); features.has_arm_neon = check_arm_hwcap(HWCAP_ASIMD); #endif // Cache size detection features.cache_line_size = get_cache_line_size(); features.l1_cache_size = get_l1_cache_size(); features.l2_cache_size = get_l2_cache_size(); features.l3_cache_size = get_l3_cache_size(); return features; } // Adaptive optimization based on detected features void optimize_for_cpu(graphite_bundle* bundle, const cpu_features* features) { // Select best CRC32 implementation if (features->has_sse42 || features->has_arm_crc32) { bundle->crc32_func = hardware_crc32; } else { bundle->crc32_func = software_crc32; } // Select best memory copy implementation if (features->has_avx2) { bundle->memcpy_func = avx2_memcpy; } else if (features->has_arm_neon) { bundle->memcpy_func = neon_memcpy; } else { bundle->memcpy_func = standard_memcpy; } // Optimize prefetch distance based on cache sizes if (features->l2_cache_size > 0) { bundle->prefetch_distance = features->l2_cache_size / (64 * 1024); bundle->prefetch_distance = max(2, min(bundle->prefetch_distance, 8)); } } ``` #### I/O Performance Optimizations ```c // Advanced I/O optimization strategies typedef struct { int queue_depth; size_t io_size; bool use_direct_io; bool use_async_io; int numa_node; } io_optimization_config; io_optimization_config determine_optimal_io_config(const graphite_bundle* bundle) { io_optimization_config config = {0}; // Determine optimal queue depth based on storage type storage_type type = detect_storage_type(bundle->file_path); switch (type) { case STORAGE_NVME: config.queue_depth = 64; config.io_size = 256 * 1024; // 256 KB config.use_direct_io = true; break; case STORAGE_SSD: config.queue_depth = 32; config.io_size = 128 * 1024; // 128 KB config.use_direct_io = true; break; case STORAGE_HDD: config.queue_depth = 8; config.io_size = 1024 * 1024; // 1 MB config.use_direct_io = false; break; case STORAGE_NETWORK: config.queue_depth = 16; config.io_size = 64 * 1024; // 64 KB config.use_direct_io = false; break; } // Enable async I/O for large bundles config.use_async_io = bundle->header.file_size > 100 * 1024 * 1024; // NUMA optimization config.numa_node = get_current_numa_node(); return config; } // Storage type detection storage_type detect_storage_type(const char* file_path) { struct stat st; if (stat(file_path, &st) != 0) { return STORAGE_UNKNOWN; } #ifdef __linux__ // Check if file is on a network filesystem struct statfs sfs; if (statfs(file_path, &sfs) == 0) { switch (sfs.f_type) { case 0x6969: // NFS case 0xFF534D42: // CIFS return STORAGE_NETWORK; } } // Check storage device characteristics char device_path[256]; if (get_device_path_for_file(file_path, device_path, sizeof(device_path))) { return analyze_block_device(device_path); } #endif return STORAGE_UNKNOWN; } ``` --- ## Migration & Compatibility ### Version Migration Strategy GRAPHITE supports automatic migration between format versions while maintaining backward compatibility: ```mermaid graph LR subgraph \"Migration Path\" A[GRAPHITE v1.0] --> B[GRAPHITE v2.0] B --> C[GRAPHITE v3.0] C --> D[Future Versions] end subgraph \"Compatibility Matrix\" E[v3.0 Loader] F[v2.x Files ✓] G[v1.x Files ✓] H[Future Files ?] E --> F E --> G E --> H end subgraph \"Migration Tools\" I[graphite migrate] J[graphite convert] K[graphite validate-migration] end ``` #### Automatic Format Migration ```c // Version migration system typedef struct { uint8_t source_version; uint8_t target_version; bool (*migrate_func)(const char* input, const char* output, const migration_options* options); const char* description; bool lossy_conversion; } version_migrator; // Migration registry static const version_migrator MIGRATORS[] = { {1, 2, migrate_v1_to_v2, \"Add compression support\", false}, {2, 3, migrate_v2_to_v3, \"Add BLAKE3 integrity trees\", false}, {1, 3, migrate_v1_to_v3, \"Direct v1 to v3 migration\", false}, }; bool migrate_bundle_version(const char* input_path, const char* output_path, uint8_t target_version, const migration_options* options) { // Detect source version uint8_t source_version = detect_bundle_version(input_path); if (source_version == 0) { return false; // Invalid or unrecognized format } if (source_version == target_version) { return copy_file(input_path, output_path); // No migration needed } // Find migration path const version_migrator* migrator = find_migrator(source_version, target_version); if (!migrator) { // Try multi-step migration return migrate_multi_step(input_path, output_path, source_version, target_version, options); } // Perform migration if (options && options->create_backup) { create_backup_file(input_path); } bool success = migrator->migrate_func(input_path, output_path, options); if (success && options && options->verify_migration) { success = verify_migration_integrity(input_path, output_path, migrator); } return success; } // Multi-step migration for complex version jumps bool migrate_multi_step(const char* input_path, const char* output_path, uint8_t source_version, uint8_t target_version, const migration_options* options) { // Build migration path using graph search migration_path* path = find_migration_path(source_version, target_version); if (!path) { return false; } char temp_files[8][256]; // Support up to 8 migration steps const char* current_input = input_path; for (int i = 0; i < path->step_count; i++) { const char* current_output; if (i == path->step_count - 1) { current_output = output_path; // Final step } else { snprintf(temp_files[i], sizeof(temp_files[i]), \"/tmp/graphite_migrate_%d_%d.tmp\", getpid(), i); current_output = temp_files[i]; } const version_migrator* migrator = path->steps[i]; if (!migrator->migrate_func(current_input, current_output, options)) { // Cleanup temp files on failure cleanup_temp_files(temp_files, i); return false; } current_input = current_output; } // Cleanup temp files cleanup_temp_files(temp_files, path->step_count - 1); return true; } ``` #### Legacy Format Support ```c // Legacy BGAF v2.0 support bool migrate_v2_to_v3(const char* input_path, const char* output_path, const migration_options* options) { // Open v2.0 bundle bgaf_v2_bundle* v2_bundle = bgaf_v2_open(input_path); if (!v2_bundle) { return false; } // Create v3.0 writer graphite_output_config output_config = { .output_path = output_path, .overwrite_existing = true }; graphite_writer* writer = graphite_writer_create(&output_config); if (!writer) { bgaf_v2_close(v2_bundle); return false; } // Configure v3.0 features if (options && options->enable_integrity) { graphite_integrity_config integrity = { .enable_hashing = true, .mandatory_verification = options->require_verification, .tree_fanout = 8 }; graphite_writer_set_integrity(writer, &integrity); } if (options && options->enable_compression) { graphite_compression_config compression = { .level = options->compression_level, .enable_dictionary = options->train_dictionary, .min_chunk_size = 64 * 1024 }; graphite_writer_set_compression(writer, &compression); } // Migrate content bool success = migrate_v2_content_to_v3(v2_bundle, writer, options); if (success) { graphite_writer_stats stats; success = graphite_writer_finalize(writer, &stats); if (success && options && options->report_statistics) { print_migration_stats(&stats, v2_bundle); } } graphite_writer_destroy(writer); bgaf_v2_close(v2_bundle); return success; } bool migrate_v2_content_to_v3(bgaf_v2_bundle* v2_bundle, graphite_writer* v3_writer, const migration_options* options) { // Create string pool from v2.0 strings graphite_string_pool_builder* strings = graphite_string_pool_builder_create(); // Migrate all strings from v2.0 for (uint32_t i = 0; i < v2_bundle->string_count; i++) { const char* str = bgaf_v2_get_string(v2_bundle, i); graphite_string_pool_add_string(strings, str); } graphite_writer_set_string_pool(v3_writer, strings); // Create root graph builder graphite_graph_builder* root_builder = graphite_graph_builder_create(); // Migrate metadata migrate_v2_metadata_to_v3(v2_bundle, root_builder); // Migrate all graphs recursively const bgaf_v2_graph* v2_root = bgaf_v2_get_root_graph(v2_bundle); if (!migrate_v2_graph_to_v3(v2_root, root_builder, v3_writer, options)) { graphite_graph_builder_destroy(root_builder); graphite_string_pool_builder_destroy(strings); return false; } // Finalize root graph graphite_writer_add_graph(v3_writer, root_builder); graphite_graph_builder_destroy(root_builder); graphite_string_pool_builder_destroy(strings); return true; } ``` ### Cross-Platform Compatibility #### Endianness Handling ```c // Robust endianness handling typedef struct { bool is_little_endian; bool needs_byte_swap; } endian_context; endian_context detect_endianness(void) { endian_context ctx; uint32_t test = 0x12345678; uint8_t* bytes = (uint8_t*)&test; ctx.is_little_endian = (bytes[0] == 0x78); ctx.needs_byte_swap = false; // GRAPHITE files are always little-endian #if __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__ ctx.needs_byte_swap = true; #endif return ctx; } // Byte swapping functions static inline uint16_t swap_uint16(uint16_t val) { return (val << 8) | (val >> 8); } static inline uint32_t swap_uint32(uint32_t val) { return ((val & 0xFF000000) >> 24) | ((val & 0x00FF0000) >> 8) | ((val & 0x0000FF00) << 8) | ((val & 0x000000FF) << 24); } static inline uint64_t swap_uint64(uint64_t val) { return ((uint64_t)swap_uint32((uint32_t)val) << 32) | (uint64_t)swap_uint32((uint32_t)(val >> 32)); } // Endian-safe reading uint32_t read_uint32_le(const void* ptr, const endian_context* ctx) { uint32_t val; memcpy(&val, ptr, sizeof(val)); if (ctx->needs_byte_swap) { val = swap_uint32(val); } return val; } uint64_t read_uint64_le(const void* ptr, const endian_context* ctx) { uint64_t val; memcpy(&val, ptr, sizeof(val)); if (ctx->needs_byte_swap) { val = swap_uint64(val); } return val; } // Safe header parsing with endianness bool parse_bundle_header_safe(const void* header_data, graphite_file_header* header) { const uint8_t* bytes = (const uint8_t*)header_data; endian_context ctx = detect_endianness(); // Magic number (always same endianness) memcpy(header->magic, bytes, 4); if (memcmp(header->magic, \"GRPH\", 4) != 0) { return false; } // Version and endianness marker header->version = bytes[4]; header->endian = bytes[5]; // Verify endianness marker if (header->endian != 0x00) { return false; // Only little-endian supported } // Parse remaining fields with endian conversion header->header_size = read_uint16_le(bytes + 6, &ctx); header->file_size = read_uint64_le(bytes + 8, &ctx); header->root_graph_index = read_uint64_le(bytes + 16, &ctx); header->string_pool_index = read_uint64_le(bytes + 24, &ctx); header->integrity_root_index = read_uint64_le(bytes + 32, &ctx); header->file_flags = read_uint32_le(bytes + 40` }``

Retry

This response paused because Claude reached its max length for a message. Hit continue to nudge Claude along.

Continue

[Claude can make mistakes. Please double-check responses.](https://support.anthropic.com/en/articles/8525154-claude-is-providing-incorrect-or-misleading-responses-what-s-going-on)